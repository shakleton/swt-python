/**********************************************************
 * C-based/Cached/Core Computer Vision Library
 * Liu Liu, 2010-02-01
 **********************************************************/

/**********************************************************
 * CCV - Neural Network Collection
 **********************************************************/

#ifndef GUARD_ccv_nnc_h
#define GUARD_ccv_nnc_h

#include <ccv.h>
#include <stddef.h>

// These are generated by cmd/build-cmd.rb
#include "cmd/ccv_nnc_cmd.h"
#include "cmd/ccv_nnc_backend.h"

/**
 * @defgroup level_0 Level-0 API
 * @{
 */

/**
 * Initialize the library.
 */
void ccv_nnc_init(void);

/** @} */

/**
 * @defgroup level_1 Level-1 API
 * @{
 */

/**
 * @defgroup level_1_cmd Commands
 * @{
 */
enum {
	// Attributes that enable symbolic graph simplification
	CCV_NNC_CMD_ATTR_PASSTHROUGH  = 0x01, /**< This doesn't compute anything, but pass the first n tensors to the output (useful for backprop that is identical). */
	CCV_NNC_CMD_ATTR_OUTPUT_ONES  = 0x02, /**< All the output tensors are 1s (unit). */
	CCV_NNC_CMD_ATTR_NULL_IS_ONES = 0x04, /**< Accept nullptr input as if these are tensors with 1s (unit). */
};

// Flags pass into cmd when executing.
enum {
	CCV_NNC_ACCUMULATE_OUTPUT = 0x01, /**< Enable accumulate outputs (unsupported). */
	CCV_NNC_ZERO_MEMORY_ALLOC = 0x02, /**< Don't allocate any extra memory for this operation. */
};

enum {
	CCV_NNC_EXEC_SUCCESS   = 0, /**< Successfully executed the command. */
	CCV_NNC_EXEC_INVALID   = -1, /**< Invalid inputs. */
	CCV_NNC_EXEC_NO_KERNEL = -2, /**< No kernel available for a given command / backend. */
	CCV_NNC_EXEC_OOM       = -3, /**< Out of memory error. */
};

/**
 * Parameters for command.
 */
typedef struct {
	struct {
		int dim[CCV_NNC_MAX_DIM_ALLOC]; /**< [size.dim] The window size for the layer. For full connect layer, it is 1 because it is 1x1 convolutional layer with count of filters */
	} size;
	union {
		struct {
			int count; /**< [convolution.count] The number of filters for convolutional layer. */
			int groups; /**< [convolution.groups] The number of groups for convolutional layer. */
		} convolution;
		struct {
			int reserved; /**< [pool.reserved] A reserved field. */
		} pool;
		struct {
			float kappa; /**< [rnorm.kappa] As of b[i] = a[i] / (rnorm.kappa + rnorm.alpha * sum(a, i - rnorm.size / 2, i + rnorm.size / 2)) ^ rnorm.beta */
			float alpha; /**< [rnorm.alpha] See **rnorm.kappa**. */
			float beta; /**< [rnorm.beta] See **rnorm.kappa**. */
		} rnorm;
		struct {
			int axis[CCV_NNC_MAX_DIM_ALLOC]; /**< [bnorm.axis[]] The axis selected to compute mean / variance. */
			int count; /**< [bnorm.count] The number of axis selected. */
			float epsilon; /**< [bnorm.epsilon] The epsilon for standard derivation. */
			int is_test; /**< [bnorm.is_test] Whether in test mode. */
			float momentum; /**< [bnorm.momentum] running_mean = running_mean * momentum + mean * (1 - momentum). */
		} bnorm;
		struct {
			int nesterov; /**< [minimize.nesterov] Nesterov accelerated gradient. */
			float rate; /**< [minimize.rate] The learning rate. */
			float scale; /**< [minimize.scale] The scale to be applied to the gradient before doing any minimization. */
			float decay; /**< [minimize.decay] This is the weight decay parameter, which represents L2 regularization after momentum applied. */
			float momentum; /**< [minimize.momentum] For SGD, this follows http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf. */
			float dampening; /**< [minimize.dampening] This usually == momentum, however, it can be changed. */
		} minimize;
		struct {
			float a[3]; /**< [blas.a[3]] BLAS scalars. */
			int count; /**< [blas.count] The number of outputs for blas layer. */
		} blas;
		struct {
			int axis[CCV_NNC_MAX_DIM_ALLOC]; /**< [reduce.axis[]] The axis selected to reduce. */
			int count; /**< [reduce.count] The number of axis selected. */
		} reduce;
		struct {
			float p; /**< [dropout.p] Dropout probability. */
		} dropout;
		void* userdata;
	};
} ccv_nnc_cmd_param_t;

/*
 * Hints for command.
 */
typedef struct {
	struct {
		int dim[CCV_NNC_MAX_DIM_ALLOC]; /**< Stride for each dimension. */
	} stride;
	struct {
		int begin[CCV_NNC_MAX_DIM_ALLOC]; /**< Padding at the beginning of a dimension. */
		int end[CCV_NNC_MAX_DIM_ALLOC]; /**< Padding at the end of a dimension. */
	} border;
} ccv_nnc_hint_t;

/**
 * Opaque pointer to a stream object.
 */
typedef struct ccv_nnc_stream_context_s ccv_nnc_stream_context_t;

typedef struct ccv_nnc_cmd_s {
	uint32_t cmd; /**< The identifier for command. */
	uint32_t backend; /**< The identifier for backend. */
	int algorithm; /**< The algorithm selector (as defined by backend). */
	ccv_nnc_cmd_param_t info; /**< The command parameters. */
	/**
	 * This has to be the same as the ccv_nnc_cmd_exec_f type.
	 * This is for type CCV_NNC_CUSTOM_FORWARD / CCV_NNC_CUSTOM_BACKWARD
	 */
	int(*exec)(const struct ccv_nnc_cmd_s cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);
} ccv_nnc_cmd_t;

/**
 * For forward functions, the input tensors and output tensors can be arbitrary.
 * However, for backward functions (backpropagation, or gradient functions in other libs),
 * the input is: 0~m-1: gradient for output tensors, 1~n: input tensors for forward functions, n+1~n+m: output tensors for forward functions,
 * the output is: 0~n-1: output gradients w.r.t. input tensors.
 * Which input / output tensors can be ignored can be specified in the cmd config structs.
 */
typedef int(*ccv_nnc_cmd_exec_f)(const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);

/**
 * The function prototype for autotune. The only difference is the max_workspace_size.
 * Whoever implement this function prototype means we handled over autotune task to the
 * command itself, you are responsible to select the best algorithm.
 * @return The selected algorithm.
 */
typedef int(*ccv_nnc_cmd_autotune_f)(const ccv_nnc_cmd_t cmd, const size_t max_workspace_size, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);

/** @} */

/**
 * @defgroup level_1_tensor Tensors
 * @{
 */

/**
 * Count the dimensionality of a tensor.
 */
static inline int ccv_nnc_tensor_nd(const int dim[CCV_NNC_MAX_DIM_ALLOC])
{
	int i;
	for (i = 0; i < CCV_NNC_MAX_DIM_ALLOC; i++)
		if (dim[i] == 0)
			return i;
	return CCV_NNC_MAX_DIM_ALLOC;
}

/**
 * Create a new tensor.
 * @param ptr If 0, nnc will allocate the tensor ourselves. Otherwise, will use the memory region referenced by 'ptr'.
 * @param params Tensor parameters.
 * @param flags Reserved flags for the allocation.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_new(const void* const ptr, const ccv_nnc_tensor_param_t params, const int flags);
/**
 * Create a new tensor on stack.
 * @param ptr If 0, nnc will allocate the tensor ourselves. Otherwise, will use the memory region referenced by 'ptr'.
 * @param params Tensor parameters.
 * @param flags Reserved flags for the allocation.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_t) ccv_nnc_tensor(const void* const ptr, const ccv_nnc_tensor_param_t params, const int flags);
/**
 * Pin the tensor memory for faster access on GPU.
 * @param tensor A tensor that we want to pin the memory.
 * @return 0 for success.
 */
int ccv_nnc_tensor_pin_memory(ccv_nnc_tensor_t* const tensor);
/**
 * Free a tensor object.
 * @param tensor The tensor to be freed.
 */
void ccv_nnc_tensor_free(ccv_nnc_tensor_t* const tensor);
/**
 * Create a tensor view. A tensor view can be non-continuous. Essentially, it provides a view into a tensor.
 * @param tensor The tensor that we want to view into.
 * @param dim The new dimension of the tensor view.
 * @param ofs The offset on each of the dimension.
 * @param inc The line size of each dimension.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_view_t*) ccv_nnc_tensor_view_new(const ccv_nnc_tensor_t* const tensor, const int dim[CCV_NNC_MAX_DIM_ALLOC], const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC]);
/**
 * Create a tensor view on stack.
 * @param tensor The tensor that we want to view into.
 * @param dim The new dimension of the tensor view.
 * @param ofs The offset on each of the dimension.
 * @param inc The line size of each dimension.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_view_t) ccv_nnc_tensor_view(const ccv_nnc_tensor_t* const tensor, const int dim[CCV_NNC_MAX_DIM_ALLOC], const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC]);
/**
 * Free a tensor view object.
 * @param tensor_view The tensor view to be freed.
 */
void ccv_nnc_tensor_view_free(ccv_nnc_tensor_view_t* const tensor_view);
/**
 * Zero out a given tensor.
 * @param tensor The tensor to be zero out.
 */
void ccv_nnc_tensor_zero(void* const tensor);
/**
 * Compare whether two tensors are equal. This will tolerant some floating point issues follow http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm
 * @param a Tensor a.
 * @param b Tensor b.
 * @return 0 if equal, -1 otherwise.
 */
CCV_WARN_UNUSED(int) ccv_nnc_tensor_eq(const ccv_nnc_tensor_t* const a, const ccv_nnc_tensor_t* const b);

/** @} */

/**
 * @addtogroup level_1_cmd
 * @{
 */

/**
 * Return a high precision time unit. What this time unit is is platform specific.
 * @return A monotonic increasing 64-bit integer w.r.t. passing of time.
 */
uint64_t ccv_nnc_cmd_mono_time(void);
/**
 * Return UTF-8 encoded name of a given command.
 * @return A UTF-8 string (pointing to a static constant).
 */
CCV_WARN_UNUSED(const char*) ccv_nnc_cmd_name(const uint32_t cmd);
/**
 * Return UTF-8 encoded name of a given backend.
 * @return A UTF-8 string (pointing to a static constant).
 */
CCV_WARN_UNUSED(const char*) ccv_nnc_cmd_backend_name(const uint32_t backend);
/**
 * Check whether a given backend is available for a given command.
 * @return 1 if it is available.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_ok(const uint32_t cmd, const uint32_t backend);
/**
 * Create a wrapped command with parameters.
 * @param cmd The command identifier.
 * @param exec If this is a CCV_NNC_CUSTOM_FORWARD / CCV_NNC_CUSTOM_BACKWARD command, this supplies the custom function.
 * @param params The parameters for the command.
 * @param flags A reserved field for flags.
 * @return A wrapped ccv_nnc_cmd_t structure.
 */
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_cmd(const uint32_t cmd, ccv_nnc_cmd_exec_f exec, const ccv_nnc_cmd_param_t params, const int flags);
/**
 * Verify whether a hint is compatible with a given command and a given input tensor parameters / output tensor parameters.
 * @param hint The hint for a given command. Hint defines things such as paddings, strides etc. for a given command.
 * @param cmd The wrapped command.
 * @param a The input tensor parameters.
 * @param b The output tensor parameters.
 * @return 1 if it passes.
 */
CCV_WARN_UNUSED(int) ccv_nnc_hint_verify(const ccv_nnc_hint_t hint, const ccv_nnc_cmd_param_t cmd, const ccv_nnc_tensor_param_t a, const ccv_nnc_tensor_param_t b);
/**
 * Automatically find the best hint for a given input / output (on forward pass only).
 * @param cmd The wrapped command.
 * @param a The input tensor parameters.
 * @param b The output tensor parameters.
 * @return Best hint we can guess.
 */
CCV_WARN_UNUSED(ccv_nnc_hint_t) ccv_nnc_hint_auto(const ccv_nnc_cmd_param_t cmd, const ccv_nnc_tensor_param_t a, const ccv_nnc_tensor_param_t b);
/**
 * Automatically find the outputs for the given inputs / hint.
 * @param cmd The wrapped command.
 * @param inputs An array of input tensor parameters.
 * @param input_size The size of input array.
 * @param hint The hint for the given command.
 * @param outputs An array for the output tensor parameters.
 * @param output_size The size of the output array.
 */
void ccv_nnc_hint_tensor_auto(const ccv_nnc_cmd_t cmd, const ccv_nnc_tensor_param_t* const inputs, const int input_size, const ccv_nnc_hint_t hint, ccv_nnc_tensor_param_t* const outputs, const int output_size);
/**
 * Find a suitable backend for a given command and tensor settings.
 * @param cmd The wrapped command.
 * @param tensor_memory The tensor memory setup (whether it is CPU or GPU).
 * @param tensor_formats The tensor layout format (NCHW, NHWC, CHWN etc.)
 * @param tensor_datatypes The datatype of a given tensor (FP32 etc.)
 * @return The backend identifier for the selected backend.
 */
CCV_WARN_UNUSED(uint32_t) ccv_nnc_cmd_find_backend(const ccv_nnc_cmd_t cmd, const int tensor_memory, const int tensor_formats, const int tensor_datatypes);
/**
 * Run autotune to find the best kernel and configuration for the given input.
 * @param cmd The original wrapped command.
 * @param max_workspace_size The maximum memory allowed for this command to execute.
 * @param hint The hint for the given command.
 * @param flags The reserved field for flags.
 * @param inputs An array of input tensors.
 * @param input_size The size of input array.
 * @param outputs An array of output tensors.
 * @param output_size The size of output array.
 * @param stream_context The stream we can do the autotune on. 0 uses default stream.
 * @return The modified cmd that contains the updated configuration.
 */
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_cmd_autotune(const ccv_nnc_cmd_t cmd, const size_t max_workspace_size, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);
/**
 * Check whether a given tensor input / output pattern can be computed by the given command.
 * bitmasks encode whether a given input tensor / output tensor available at a position.
 * @param cmd The wrapped command to check.
 * @param input_size The intended size of the input tensor array.
 * @param output_size The intended size of the output tensor array.
 * @param input_bitmasks The input tensor array encoding in bitmap, 0: no tensor, 1: has a tensor.
 * @param input_bitmask_size The size of the input bitmask array.
 * @param output_bitmasks The output tensor array encoding in bitmap.
 * @param output_bitmask_size The size of the output bitmask array.
 * @return 1 if the command can be executed with the given input / output pattern.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_bitmask(const ccv_nnc_cmd_t cmd, const int input_size, const int output_size, const uint64_t* const input_bitmasks, const int input_bitmask_size, const uint64_t* const output_bitmasks, const int output_bitmask_size);
/**
 * Execute a given command.
 * @param cmd The wrapped command to be executed.
 * @param hint The hint provided for the command.
 * @param flags A reserved field for flags.
 * @param inputs The input tensor array.
 * @param input_size The size of input tensor array.
 * @param outputs The output tensor array.
 * @param output_size The size of output tensor array.
 * @param stream_context The stream which the command will be executed upon.
 * @return CCV_NNC_EXEC_SUCCESS if succeed.
 */
int ccv_nnc_cmd_exec(const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);
/**
 * Check whether the command is a forward pass or not.
 * @param cmd The wrapped command.
 * @return 1 if it is a forward pass.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_is_forward(const ccv_nnc_cmd_t cmd);
/**
 * Check whether the command is a backward pass or not.
 * @param cmd The wrapped command.
 * @return 1 if it is a backward pass.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_is_backward(const ccv_nnc_cmd_t cmd);
/**
 * Check this command against listed attributes.
 * @param cmd The wrapped command.
 * @param flags The flags to check against the command (unsupported).
 * @return 1 if the flag is supported by the command.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_attr(const ccv_nnc_cmd_t cmd, const int flags);
/**
 * Check whether this command allow inplace operation against a particular input and output (index from 0).
 * @param cmd The wrapped command.
 * @param input_idx The index of the input tensor we want to check.
 * @param input_size The total number of inputs.
 * @param output_idx the index of the output tensor we want to check.
 * @param output_size The total number of outputs.
 * @return 1 if the input tensor can be used as the output tensor.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_allow_inplace(const ccv_nnc_cmd_t cmd, const int input_idx, const int input_size, const int output_idx, const int output_size);
/**
 * Check whether this command need to enforce inplace operation against a particular input and output (index from 0).
 * @param cmd The wrapped command.
 * @param input_idx The index of the input tensor we want to check.
 * @param input_size The total number of inputs.
 * @param output_idx the index of the output tensor we want to check.
 * @param output_size The total number of outputs.
 * @return 1 if the input tensor is required to be used as the output tensor.
 */
CCV_WARN_UNUSED(int) ccv_nnc_cmd_enforce_inplace(const ccv_nnc_cmd_t cmd, const int input_idx, const int input_size, const int output_idx, const int output_size);

/** @} */

/**
 * @defgroup level_1_stream Streams
 * @{
 */

// Control flow constructs
// Follow heavily based along CUDA's stream / event idea.
enum {
	CCV_STREAM_CONTEXT_CPU = 0x1, /**< A CPU based stream context (unsupported). */
	CCV_STREAM_CONTEXT_GPU = 0x2, /**< A GPU based stream context. */
};
#define CCV_STREAM_GET_CONTEXT(type) ((type) & 0x3)
#define CCV_STREAM_GET_DEVICE(type) ((type) & 0xff00)
#define CCV_STREAM_GET_DEVICE_ID(type) (CCV_STREAM_GET_DEVICE(type) >> 8)
#define CCV_STREAM_SET_DEVICE_ID(type, device_id) (type) = (((type) & ~0xfff00) | (((device_id) & 0xfff) << 8))
/**
 * Create a new stream context.
 * @param type A combination of CPU / GPU and DEVICE_ID.
 * @return The newly created stream context.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_nnc_stream_context_new(const int type);
/**
 * Get the type of the stream context.
 * @param stream_context The stream context we want to inspect.
 * @return The type of the stream context.
 */
CCV_WARN_UNUSED(int) ccv_nnc_stream_context_type(const ccv_nnc_stream_context_t* const stream_context);
/**
 * Get a stream context local workspace memory. This memory region will be reused
 * the next time when you call this method on the same stream context.
 * @param stream_context The stream context which provides the workspace memory.
 * @param workspace_size The size of the workspace memory.
 * @param mem The memory type of the said workspace memory (GPU or CPU).
 * @return A pointer to the workspace memory.
 */
CCV_WARN_UNUSED(void*) ccv_nnc_stream_context_get_workspace(ccv_nnc_stream_context_t* const stream_context, const size_t workspace_size, const int mem);
/**
 * Deallocate any workspace memory on the stream context.
 * @param stream The stream context to drain workspace memory.
 */
void ccv_nnc_stream_context_drain(ccv_nnc_stream_context_t* const stream);
/**
 * Wait until all tasks submitted (command, graph run etc.) on the stream context
 * completed.
 * @param stream The stream context to wait.
 */
void ccv_nnc_stream_context_wait(const ccv_nnc_stream_context_t* const stream);
/**
 * Deallocate the stream context.
 * @param stream_context The stream context to be destroyed.
 */
void ccv_nnc_stream_context_free(ccv_nnc_stream_context_t* const stream_context);

/**
 * Opaque pointer to the signal object.
 */
typedef struct ccv_nnc_stream_signal_s ccv_nnc_stream_signal_t;

/**
 * Create a new stream signal.
 * @param type A composed type denotes whether it associated with a GPU or CPU stream context, and on which device.
 * @return The newly created stream signal.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_signal_t*) ccv_nnc_stream_signal_new(const int type);
/**
 * Emit a signal on a stream.
 * @param stream The stream context where the signal will be emitted.
 * @param signal The signal to be emitted. It has to be on the same device as the stream.
 */
void ccv_nnc_stream_context_emit_signal(ccv_nnc_stream_context_t* const stream, ccv_nnc_stream_signal_t* const signal);
/**
 * Wait a signal on a stream.
 * @param stream The stream context that will be blocked by the signal.
 * @param signal The signal to be waited. It can be on a different device of the stream.
 */
void ccv_nnc_stream_context_wait_signal(const ccv_nnc_stream_context_t* const stream, const ccv_nnc_stream_signal_t* const signal);
/**
 * Get on which stream context this signal is going to be emitted on.
 * @param signal The signal we want to inspect.
 * @return The most recent stream context you called ccv_nnc_stream_context_emit_signal with.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_nnc_stream_signal_get_emitter(const ccv_nnc_stream_signal_t* const signal);
/**
 * Get a signal handle for a stream context, you can then wait this signal for this stream.
 * This is handy if you want to have a stream context specific signal that can be identified
 * with an identifier, and don't want to maintain it yourself. The returned signal cannot
 * be freed and managed by the stream itself.
 * @param stream The stream where the signal will be bind to.
 * @param identifier The identifier for this signal.
 * @return A signal that is binded to this stream and ready to be used.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_signal_t*) ccv_nnc_stream_context_get_signal(ccv_nnc_stream_context_t* const stream, const int64_t identifier);
/**
 * Deallocate the signal.
 * @param signal The signal to be destroyed.
 */
void ccv_nnc_stream_signal_free(ccv_nnc_stream_signal_t* const signal);
/**
 * Return number of devices.
 * @param type The type of devices (CCV_NNC_STREAM_CONTEXT_GPU / CCV_NNC_STREAM_CONTEXT_CPU)
 * @return The number of devices.
 */
CCV_WARN_UNUSED(int) ccv_nnc_device_count(const int type);
/**
 * Remap a source device as the destination device.
 * @param type The type of devices (CCV_NNC_STREAM_CONTEXT_GPU / CCV_NNC_STREAM_CONTEXT_CPU)
 * @param source The original device id.
 * @param destination The new device id.
 * @return 0 if the device remap is successful, -1 if it is not.
 */
CCV_WARN_UNUSED(int) ccv_nnc_device_remap(const int type, const int source, const int destination);
/**
 * The neighbor discovery function that will be called with the device id.
 */
typedef ccv_nnc_stream_context_t*(*ccv_nnc_stream_context_neighbor_discovery_f)(const int device_id, void* const context);
/**
 * Set the neighbor stream context discovery mechanism. This method exposes how
 * neighbor should be defined per stream context. This method is useful for
 * commands that operates cross devices and need to find the correct stream
 * context for these devices. Stream context itself is bounded to one device
 * only.
 * @param stream_context The stream context that bounds to a discovery mechanism.
 * @param discovery The neighbor discovery function to invoke.
 * @param context The associated context with the neighbor discovery function.
 */
void ccv_nnc_stream_context_set_neighbor_discovery(ccv_nnc_stream_context_t* const stream_context, ccv_nnc_stream_context_neighbor_discovery_f discovery, void* const context);
/**
 * Find a neighbor stream context on a given device id for current stream context.
 * @param stream_context The stream context which we will look for neighbors.
 * @param device_id On which device the stream context may exist.
 * @return 0 if no stream context found. Otherwise, return the stream context on that device.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_nnc_stream_context_find_neighbor(ccv_nnc_stream_context_t* const stream_context, const int device_id);

/** @} */

/** @} */

/**
 * @defgroup level_2 Level-2 API
 * @{
 */

/**
 * @defgroup level_2_essentials Essentials
 * @{
 */

enum {
	CCV_NNC_SHORT_DOT_GRAPH = 0x0, /**< Display a simplified graph. */
	CCV_NNC_LONG_DOT_GRAPH  = 0x1, /**< Display a graph that contains all information. */
};

/**
 * Opaque pointer holds the concrete graph representation.
 */
typedef struct ccv_nnc_graph_s ccv_nnc_graph_t;

/**
 * The opaque on stack object hold a reference to an execution node within a graph.
 */
typedef struct {
	int32_t d; // This is int because sometimes I piggy-back on negatives to carry out some internal computations.
	ccv_nnc_graph_t* graph;
} ccv_nnc_graph_exec_t;

#define CCV_NO_GRAPH_EXEC(exec) ((exec).graph == 0)

/**
 * Create an empty graph.
 * Note that all graph mutation methods are not thread-safe.
 * You should only operate the graph in serial fashion.
 * @return An opaque ccv_nnc_graph_t pointer.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_t*) ccv_nnc_graph_new(void);
/**
 * Create a node with specific command execution, as well as its inputs & outputs.
 * Underlying, the graph maintains the backing object for the node, and all you get is
 * a on-stack object to index the backing object from the graph.
 * @param graph The concrete graph.
 * @param cmd The wrapped command.
 * @param hint The hint for this command.
 * @param inputs The input tensors array.
 * @param input_size The size of input tensors array.
 * @param outputs The output tensors array.
 * @param output_size The size of output tensors array.
 * @return An on-stack object that references a execution node.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_new(ccv_nnc_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size);
/**
 * Set the command for an existing execution node.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param cmd The new wrapped command.
 */
void ccv_nnc_graph_exec_set(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const ccv_nnc_cmd_t cmd);
/**
 * Set hint for an existing execution node.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param hint The new hint.
 */
void ccv_nnc_graph_exec_set_hint(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const ccv_nnc_hint_t hint);
/**
 * Set input / output tensors for an existing execution node.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param inputs The input tensors array.
 * @param input_size The size of input tensors array.
 * @param outputs The output tensors array.
 * @param output_size The size of output tensors array.
 */
void ccv_nnc_graph_exec_set_io(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size);
/**
 * Concatenate input graph nodes with an output graph node to create a new graph.
 * @param graph The concrete graph.
 * @param source The execution node reference to connect.
 * @param destination The execution node reference connect to.
 * @return Non-zero if cannot concat successfully.
 */
int ccv_nnc_graph_exec_concat(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t source, const ccv_nnc_graph_exec_t destination);
/**
 * Disconnect input graph nodes with an output graph nodes in this graph.
 * @param graph The concrete graph.
 * @param source The execution node reference to disconnect.
 * @param destination The execution node reference disconnect to.
 * @return Non-zero if cannot disjoin successfully.
 */
int ccv_nnc_graph_exec_disjoin(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t source, const ccv_nnc_graph_exec_t destination);
/**
 * Count number of exec in the graph.
 * @param graph The concrete graph.
 * @return The number of execution nodes in the graph.
 */
int ccv_nnc_graph_exec_count(const ccv_nnc_graph_t* const graph);
/**
 * Generate output that can be parsed by GraphViz (DOT language).
 * @param graph The concrete graph.
 * @param flags Either CCV_NNC_SHORT_DOT_GRAPH or CCV_NNC_LONG_DOT_GRAPH
 * @param out The output file stream.
 */
void ccv_nnc_graph_dot(const ccv_nnc_graph_t* const graph, const int flags, FILE* out);
/**
 * Run the autotune function on all execution node, and assign back with the optimized commands.
 * @param graph The concrete graph.
 * @param max_workspace_size The maximum allowed extra memory usage.
 * @param flags A reserved field for flags.
 * @param sources The source execution nodes to begin. 0 uses default sources.
 * @param source_size The size of source execution nodes.
 * @param destinations The destination execution nodes which we end. 0 uses default destinations.
 * @param destination_size The size of destination execution nodes.
 */
void ccv_nnc_graph_autotune(ccv_nnc_graph_t* const graph, const size_t max_workspace_size, const int flags, const ccv_nnc_graph_exec_t* const sources, const int source_size, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);
/**
 * Make the graph topsorted, thus, do a topological sort so when run the graph, no additional memory will be allocated.
 * Otherwise when we run the graph, we need to allocate some memory on heap to faciliate.
 * @param graph The concrete graph.
 * @param exec_cvt The execution node assignments will change, and you can give an array to know the changes.
 * @param exec_cvt_size The provided conversion array size.
 */
void ccv_nnc_graph_topsort(ccv_nnc_graph_t* const graph, int* const exec_cvt, const int exec_cvt_size);
/**
 * Allocate extra streams to make this graph parallel runnable. Note this requires the graph to be topsorted.
 * After this is done, you can schedule a graph either on its default stream, or a new stream.
 * @param graph The concrete graph.
 * @param stream_type The type of stream context we are going to use.
 */
void ccv_nnc_graph_static_schedule(ccv_nnc_graph_t* const graph, const int stream_type);
/**
 * Query the default stream for a given graph.
 * @param graph The concrete graph.
 * @return The default stream context.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_nnc_graph_default_stream(const ccv_nnc_graph_t* const graph);
/**
 * Set default sources for a give graph.
 * @param graph The concrete graph.
 * @param sources The source execution nodes to begin.
 * @param source_size The size of source execution nodes.
 */
void ccv_nnc_graph_set_sources(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t* const sources, const int source_size);
/**
 * Get the default source execution nodes pointer.
 * @param graph The concrete graph.
 * @return A pointer to an array of default source execution nodes.
 */
ccv_nnc_graph_exec_t* ccv_nnc_graph_sources(const ccv_nnc_graph_t* const graph);
/**
 * Get the number of default source execution nodes.
 * @param graph The concrete graph.
 * @return The number of default source execution nodes.
 */
int ccv_nnc_graph_source_size(const ccv_nnc_graph_t* const graph);
/**
 * Set default destinations for a give graph.
 * @param graph The concrete graph.
 * @param destinations The destination execution nodes which we end.
 * @param destination_size The size of destination execution nodes.
 */
void ccv_nnc_graph_set_destinations(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);
/**
 * Get the default destination execution nodes pointer.
 * @param graph The concrete graph.
 * @return A pointer to an array of default destination execution nodes.
 */
ccv_nnc_graph_exec_t* ccv_nnc_graph_destinations(const ccv_nnc_graph_t* const graph);
/**
 * Get the number of default destination execution nodes.
 * @param graph The concrete graph.
 * @return The number of default destination execution nodes.
 */
int ccv_nnc_graph_destination_size(const ccv_nnc_graph_t* const graph);
/**
 * This graph, and its relevant auxiliary objects (opaque to user) are deallocated.
 * @param graph The concrete graph.
 */
void ccv_nnc_graph_free(ccv_nnc_graph_t* const graph);
/**
 * Opaque pointer to the tape of tensors. The tape are used by the while loop.
 */
typedef struct ccv_nnc_tensor_tape_s ccv_nnc_tensor_tape_t;
/**
 * Execute a computation graph with all bells and whistles. Need to supply a tensor tape if it contains backward pass
 * for while loop or branches. With tensor tape, the tensors are versioned, so you can "backpropagate through time".
 * @param graph The concrete graph.
 * @param tensor_tape An opaque tensor tape object to "backpropagate through time".
 * @param stream_context Which stream this graph will be executed upon.
 * @param flags A reserved field for flags.
 * @param sources The source execution nodes array.
 * @param source_size The size of source execution nodes array. 0 uses default sources.
 * @param destinations The destination execution nodes array.
 * @param destination_size The size of destination execution nodes array. 0 uses default destinations.
 * @return CCV_NNC_EXEC_SUCCESS if succeed.
 */
int ccv_nnc_graph_run(ccv_nnc_graph_t* const graph, ccv_nnc_tensor_tape_t* const tensor_tape, ccv_nnc_stream_context_t* const stream_context, const int flags, const ccv_nnc_graph_exec_t* const sources, const int source_size, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);

/** @} */

/**
 * @defgroup level_2_others Others
 * @{
 */

/**
 * Set input / output flags for an existing execution node.
 * This must be called after set_io, set additional flags for tensors related to this exec.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param input_flags The input flags array.
 * @param input_flag_size the size of input flags array, should be the same as input tensors array (or 0).
 * @param output_flags The output flags array.
 * @param output_flag_size the size of output flags array, should be the same as output tensors array (or 0).
 */
void ccv_nnc_graph_exec_set_io_flags(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const int* const input_flags, const int input_flag_size, const int* const output_flags, const int output_flag_size);
/**
 * Set the peer reference for exec. In backward pass, an execution node's peer node is the forward pass node.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param peer_exec The peer execution node reference.
 */
void ccv_nnc_graph_exec_set_peer(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const ccv_nnc_graph_exec_t peer_exec);
/**
 * Add tensor pair that can be used to "carry over". (carry over: passing a tensor from current loop to the next loop).
 * @param graph The concrete graph.
 * @param from The tensor we have output in this loop.
 * @param to The tensor we will use as input in the next loop.
 */
void ccv_nnc_graph_add_carry_over(ccv_nnc_graph_t* const graph, const ccv_nnc_tensor_t* const from, const ccv_nnc_tensor_t* const to);
/**
 * Updates are the tensors that not directly involved in the computation, but its pointers need to get updated
 * along with this exec, thus need to be "update" to other exec nodes.
 * @param graph The concrete graph.
 * @param exec The execution node reference.
 * @param update The tensor need to be updated along the execution node.
 */
void ccv_nnc_graph_exec_add_update(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_tensor_t* const update);

/** @} */

/** @} */

/**
 * @defgroup level_3 Level-3 API
 * @{
 */

/**
 * @defgroup level_3_essentials Essentials
 * @{
 */

/**
 * Opaque pointer to the symbolic graph object.
 */
typedef struct ccv_nnc_symbolic_graph_s ccv_nnc_symbolic_graph_t;

/**
 * Opaque pointer to an arena of allocated tensors.
 */
typedef struct ccv_nnc_tensor_arena_s ccv_nnc_tensor_arena_t;

/**
 * Opaque pointer to an arena of allocated execs.
 */
typedef struct ccv_nnc_graph_exec_arena_s ccv_nnc_graph_exec_arena_t;

/**
 * On stack object references a tensor symbol in the symbolic graph.
 */
typedef struct {
	int32_t d;
	const ccv_nnc_symbolic_graph_t* graph;
} ccv_nnc_tensor_symbol_t;

/**
 * On stack object references a execution node symbol in the symbolic graph.
 */
typedef struct {
	int32_t d;
	const ccv_nnc_symbolic_graph_t* graph;
} ccv_nnc_graph_exec_symbol_t;

enum {
	CCV_NNC_TENSOR_SYMBOL_INIT_ZEROS = 0x01, /**< Initialize underlying tensor for the symbol with zeros */
	CCV_NNC_TENSOR_SYMBOL_INIT_ONES = 0x02, /**< Initialize underlying tensor for the symbol with ones */
	CCV_NNC_TENSOR_SYMBOL_TAPE_VAR = 0x04, /**< Mark this as a tape variable (it cannot be folded, will contain flag CCV_TAPE_ALLOC) */
	// The one below is special.
	CCV_NNC_TENSOR_SYMBOL_DEAD = 0x80000000, /**< Mark this tensor symbol as dead, any future usage will cause assertion */
};

#define CCV_NNC_TENSOR_SYMBOL_IS_DEAD(x) ((x) & CCV_NNC_TENSOR_SYMBOL_DEAD)

enum {
	CCV_NNC_GRAPH_EXEC_DEAD = 0x1, /**< Mark this node as dead. */
	CCV_NNC_GRAPH_EXEC_P_WHILE = 0x10, /**< Mark this node keyword is while */
	CCV_NNC_GRAPH_EXEC_CASE_OF = 0x20, /**< Mark this node keyword is case_of */
};

#define CCV_NNC_GRAPH_EXEC_IS_DEAD(x) ((x) & CCV_NNC_GRAPH_EXEC_DEAD)
#define CCV_NNC_GRAPH_REF(x) ((x)->_heap_graph_ref ? (x)->_heap_graph_ref : (x)->_inline_graph_ref)

enum {
	CCV_NNC_NO_TENSOR_SYMBOL = -1, /**< Special symbol reference for no tensor symbol. */
	CCV_NNC_WHILE_COUNT_TENSOR_SYMBOL = -2, /**< Special symbol reference for while loop count tensor. */
};

enum {
	CCV_NNC_NO_GRAPH_EXEC_SYMBOL = -1, /**< Special symbol reference for no exec symbol. */
};

#define CCV_NNC_IS_WHILE_COUNT_TENSOR_SYMBOL(d) (((uint32_t)(d) & 0xf) == 0xe)

/**
 * A data structure to pass in a pair of tensor symbols.
 */
typedef struct {
	ccv_nnc_tensor_symbol_t source; /**< The 'from' tensor symbol. */
	ccv_nnc_tensor_symbol_t destination; /**< The 'to' tensor symbol. */
} ccv_nnc_tensor_symbol_map_t;

/**
 * Create a new empty symbolic graph. It is an opaque data structure that maintains the whole graph of computation in its symbolic form.
 * Note that all graph mutation methods are not thread-safe. You should only operate the graph in serial fashion.
 */
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_new(void);
/**
 * Create an tensor symbol (thus, with no actual memory space allocation) in a symbolic graph.
 * @param graph The symbolic graph.
 * @param info The tensor parameters.
 * @param name The name of the tensor symbol, it is optional.
 * @return A tensor symbol reference.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_param_t info, const char* const name);
/**
 * Create an alias to the tensor symbol as tensor view (thus, pointing to the same memory region, but with a different header info and offset).
 * @param graph The symbolic graph.
 * @param tensor_symbol The tensor symbol we are going to reference to.
 * @param ofs The offset on each of the dimension.
 * @param inc The line size of each dimension.
 * @param info The tensor parameters for the new alias.
 * @param name The name of the tensor symbol alias, it is optional.
 * @return A tensor symbol alias reference.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_alias_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol, const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC], const ccv_nnc_tensor_param_t info, const char* const name);
/**
 * Manually delete a tensor symbol off the symbolic graph.
 * @param graph The symbolic graph.
 * @param tensor The tensor symbol reference.
 */
void ccv_nnc_tensor_symbol_free(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_tensor_symbol_t tensor);
/**
 * Create a graph execution node (an operation that takes a set of inputs and generates a set of outputs).
 * @param graph The symbolic graph.
 * @param cmd The wrapped command.
 * @param inputs The input tensor symbols array.
 * @param input_size The size of input tensor symbols array.
 * @param outputs The output tensor symbols array.
 * @param output_size The size of output tensor symbols array.
 * @param name The name of this execution node, optional.
 * @return The execution node symbol reference.
 */
ccv_nnc_graph_exec_symbol_t ccv_nnc_graph_exec_symbol_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const char* const name);
/**
 * ccv_nnc_graph_exec_symbol_new defaults to use `ccv_nnc_hint_auto` find the best hints for a set of inputs / outputs.
 * However, you can also set your own hints.
 * @param graph The symbolic graph.
 * @param exec The execution node symbol reference.
 * @param hint The hint for the command.
 */
void ccv_nnc_graph_exec_symbol_set_hint(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, const ccv_nnc_hint_t hint);
/**
 * Manually delete a exec symbol off the symbolic graph.
 * @param graph The symbolic graph.
 * @param symbol The execution node symbol reference.
 */
void ccv_nnc_graph_exec_symbol_free(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_graph_exec_symbol_t symbol);
enum {
	CCV_NNC_AUTOGEN_ALL_EXECS = 0x1, /**< Automatic concatenation for all execution nodes */
	CCV_NNC_AUTOGEN_SOURCES_AND_DESTINATIONS = 0x2, /**< Automatically find all source and destination nodes. */
};
/**
 * Automatic concatenate these nodes together based on its inputs / outputs.
 * Imagining this is to generate the execution flow based on input tensors and output tensors.
 * nil for execs and 0 for exec_size means to loop over all the execs on the graph and autogen.
 * @param graph The symbolic graph.
 * @param execs The execution nodes array.
 * @param exec_size The size of execution nodes array.
 * @param flags The flags determines what operations to perform when concatenating.
 * @return non-zero if cannot figure out.
 */
int ccv_nnc_graph_exec_symbol_autogen(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const execs, const int exec_size, const int flags);
/**
 * Set the default sources for a symbolic graph.
 * @param graph The symbolic graph.
 * @param sources The source execution nodes array.
 * @param source_size The size of source execution nodes array.
 */
void ccv_nnc_symbolic_graph_set_sources(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size);
/**
 * Get the pointer to the default sources.
 * @param graph The symbolic graph.
 * @return The pointer to the source execution nodes array.
 */
ccv_nnc_graph_exec_symbol_t* ccv_nnc_symbolic_graph_sources(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * Get the size of the default source nodes array.
 * @param graph The symbolic graph.
 * @return The size of the default source nodes array.
 */
int ccv_nnc_symbolic_graph_source_size(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * Set the default destinations for a symbolic graph.
 * @param graph The symbolic graph.
 * @param destinations The destination execution nodes array.
 * @param destination_size The size of destination execution nodes array.
 */
void ccv_nnc_symbolic_graph_set_destinations(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);
/**
 * Get the pointer to the default destinations.
 * @param graph The symbolic graph.
 * @return The pointer to the destinationsexecution nodes array.
 */
ccv_nnc_graph_exec_symbol_t* ccv_nnc_symbolic_graph_destinations(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * Get the size of the default destination nodes array.
 * @param graph The symbolic graph.
 * @return The size of the default destination nodes array.
 */
int ccv_nnc_symbolic_graph_destination_size(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * Generate output that can be parsed by GraphViz (DOT language).
 * @param graph The symbolic graph.
 * @param flags Either CCV_NNC_SHORT_DOT_GRAPH or CCV_NNC_LONG_DOT_GRAPH
 * @param out The output file stream.
 */
void ccv_nnc_symbolic_graph_dot(const ccv_nnc_symbolic_graph_t* const graph, const int flags, FILE* out);

/**
 * The data structure to wrap a tensor symbol and a concrete tensor together.
 */
typedef struct {
	ccv_nnc_tensor_symbol_t symbol;
	const ccv_nnc_tensor_t* tensor;
} ccv_nnc_tensor_bind_t;

/**
 * Compile a symbolic graph into a graph that can be executed, and a set of tensors (opaque data structure tensor arena) are allocated based on which tensor symbols are the input and which are the outputs. The tensor allocation is done to minimize the required storage.
 * tensor_binds provide custom binding for these tensors. You still responsible to manage the life-time of these tensors.
 * outputs marks the tensor symbols that need to be kept til the end of the graph.
 * @param graph The symbolic graph.
 * @param tensor_binds The binding array (a tensor symbol and a concrete tensor). We replace everywhere that uses the tensor symbol with the concrete tensor.
 * @param tensor_bind_size The size of the binding array.
 * @param outputs The output tensor symbols that we want to keep the value.
 * @param output_size The size of the output tensor symbols array.
 * @param sources The sources for the graph.
 * @param source_size The size of the sources array. 0 to use default sources.
 * @param destinations The destinations for the graph.
 * @param destination_size The size of the destinations array. 0 to use default destinations.
 * @param graph_ref The pointer to store the generated concrete graph.
 * @param tensor_arena_ref The pointer to store ccv_nnc_tensor_arena_t.
 * @param graph_exec_arena_ref The pointer to store ccv_nnc_graph_exec_arena_t.
 */
void ccv_nnc_symbolic_graph_compile(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_bind_t* const tensor_binds, const int tensor_bind_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size, ccv_nnc_graph_t** const graph_ref, ccv_nnc_tensor_arena_t** const tensor_arena_ref, ccv_nnc_graph_exec_arena_t** const graph_exec_arena_ref);
/**
 * Free the symbolic graph and its associated memory. Note that if you compiled a graph / tensor arena out of this symbolic graph, these won't be free'd.
 * @param graph The symbolic graph.
 */
void ccv_nnc_symbolic_graph_free(ccv_nnc_symbolic_graph_t* const graph);
/**
 * Find corresponding tensor by a symbol from the tensor arena.
 * @param tensor_arena The tensor arena object generated through compilation,
 * @param symbol The tensor symbol reference. Because tensor symbol reference is on stack. It can still be used even the original symbolic graph is free'd.
 * @return A concrete tensor from the tensor arena.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_from_symbol(const ccv_nnc_tensor_arena_t* const tensor_arena, const ccv_nnc_tensor_symbol_t symbol);
/**
 * Bind a tensor to a symbol. You still responsible to manage the life-time of the tensor to make sure it is not freed until everything is done.
 * @param tensor_arena The tensor arena object generated through compilation.
 * @param symbol The tensor symbol reference. Because tensor symbol reference is on stack. It can still be used even the original symbolic graph is free'd.
 * @param tensor The new tensor to bind to.
 */
void ccv_nnc_tensor_bind_symbol(const ccv_nnc_tensor_arena_t* const tensor_arena, const ccv_nnc_tensor_symbol_t symbol, const ccv_nnc_tensor_t* const tensor);
/**
 * Free the opaque tensor arena structure.
 * @param tensor_arena The tensor arena object generated through compilation.
 */
void ccv_nnc_tensor_arena_free(ccv_nnc_tensor_arena_t* const tensor_arena);
/**
 * Find corresponding graph exec by a exec symbol from graph exec arena.
 * @param graph_exec_arena The graph execution node arena object generated through compilation,
 * @param symbol The execution node symbol reference. Because execution node symbol reference is on stack. It can still be used even the original symbolic graph is free'd.
 * @return A execution node reference to the concrete graph.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_from_symbol(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena, const ccv_nnc_graph_exec_symbol_t symbol);
/**
 * Return the node that can drive all the source nodes from the compilation.
 * @param graph_exec_arena The graph execution node arena object generated through compilation,
 * @return A execution node reference that is the source.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_source(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena);
/**
 * Return the node that can drain all the destination nodes from the compilation.
 * @param graph_exec_arena The graph execution node arena object generated through compilation,
 * @return A execution node reference that is the destination.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_destination(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena);
/**
 * Free the opaque graph exec arena structure.
 * @param graph_exec_arena The graph execution node arena object generated through compilation,
 */
void ccv_nnc_graph_exec_arena_free(ccv_nnc_graph_exec_arena_t* const graph_exec_arena);
/**
 * Write symbolic graph to disk, along with some binding tensors.
 * @param graph The symbolic graph.
 * @param tensor_binds The binding array (pair of tensor symbol and concrete tensor).
 * @param tensor_bind_size The size of the binding array.
 * @param fn The file name.
 */
void ccv_nnc_symbolic_graph_write(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_bind_t* const tensor_binds, const int tensor_bind_size, const char* const fn);
/**
 * Read symbolic graph from disk, with some binding tensors.
 * @param fn The file name.
 * @param graph_ref The pointer to store symbolic graph.
 * @param tensor_binds_ref The pointer to store the binding array.
 * @param tensor_bind_size_ref The pointer to store the size of the binding array.
 */
void ccv_nnc_symbolic_graph_read(const char* const fn, ccv_nnc_symbolic_graph_t** const graph_ref, ccv_nnc_tensor_bind_t** const tensor_binds_ref, int* const tensor_bind_size_ref);

/** @} */

/**
 * @defgroup level_3_others Others
 * @{
 */

/**
 * Return the symbol it alias to.
 * @param graph The symbolic graph.
 * @param tensor_symbol The tensor symbol alias.
 * @return A tensor symbol reference to the original tensor symbol. If this symbol has no reference, return NO_SYMBOL (.graph = 0)
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_alias_to(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol);
/**
 * Set the tensor symbol parameters.
 * @param graph The symbolic graph.
 * @param tensor The tensor symbol reference.
 * @param info The new tensor parameters.
 * @return non-zero if encountered errors.
 */
int ccv_nnc_tensor_symbol_set(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor, const ccv_nnc_tensor_param_t info);
/**
 * Get the parameters for a tensor symbol.
 * @param graph The symbolic graph.
 * @param tensor The tensor symbol reference.
 * @return The tensor parameters.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_param_t) ccv_nnc_tensor_symbol_params(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor);
/**
 * Set the flags for this tensor symbol. The flags are only used for symbol, not for tensor.
 * @param graph The symbolic graph.
 * @param tensor The tensor symbol reference.
 * @param flags A reserved field for flags.
 */
int ccv_nnc_tensor_symbol_set_flags(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor, const int flags);
/**
 * Get all the flags for a tensor.
 * @param graph The symbolic graph.
 * @param tensor The tensor symbol reference.
 */
CCV_WARN_UNUSED(int) ccv_nnc_tensor_symbol_flags(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor);
/**
 * Set the cmd of this exec symbol.
 * @param graph The symbolic graph.
 * @param exec The execution node symbol reference.
 * @param cmd The new wrapped command.
 */
void ccv_nnc_graph_exec_symbol_set(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, const ccv_nnc_cmd_t cmd);
/**
 * Return the command on this exec symbol.
 * @param graph The symbolic graph.
 * @param exec The execution node symbol reference.
 * @return The wrapped command.
 */
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_graph_exec_symbol_cmd(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec);
/**
 * Set the inputs / outputs for a exec symbol.
 * @param graph The symbolic graph.
 * @param exec The execution node symbol reference.
 * @param inputs The input tensor symbols array.
 * @param input_size The size of input tensor symbols array.
 * @param outputs The output tensor symbols array.
 * @param output_size The size of output tensor symbols array.
 */
void ccv_nnc_graph_exec_symbol_set_io(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size);
/**
 * Manually concatenate input node with an output graph node.
 * @param graph The symbolic graph.
 * @param source The source execution node symbol to connect.
 * @param destination The destination execution node symbol connect to.
 * @return non-zero if cannot concat successfully.
 */
int ccv_nnc_graph_exec_symbol_concat(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t source, const ccv_nnc_graph_exec_symbol_t destination);
/**
 * Manually disconnect input node with an output graph node for this graph.
 * @param graph The symbolic graph.
 * @param source The source execution node symbol to disconnect.
 * @param destination The destination execution node symbol disconnect to.
 * @return non-zero if cannot disjoin successfully.
 */
int ccv_nnc_graph_exec_symbol_disjoin(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t source, const ccv_nnc_graph_exec_symbol_t destination);
/**
 * Number of exec symbols.
 * @param graph The symbolic graph.
 */
CCV_WARN_UNUSED(int) ccv_nnc_graph_exec_symbol_count(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * Substitution function. Given an execution node symbol and a command, return a new command.
 */
typedef ccv_nnc_cmd_t(*ccv_nnc_symbolic_graph_subst_f)(const ccv_nnc_graph_exec_symbol_t symbol, const ccv_nnc_cmd_t cmd);
/**
 * Generate a duplicate of the provided graph.
 * While generating the duplicate, it calls the function pointer to re-process the node type.
 * @param graph The symbolic graph.
 * @param subst The substitution function.
 * @return The duplicated symbolic graph.
 */
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_dup(const ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_symbolic_graph_subst_f subst);
/**
 * Number of tensor symbols.
 * @param graph The symbolic graph.
 */
CCV_WARN_UNUSED(int) ccv_nnc_tensor_symbol_count(const ccv_nnc_symbolic_graph_t* const graph);
/**
 * The opaque structure to iterate over graph.
 */
typedef struct ccv_nnc_symbolic_graph_iter_s ccv_nnc_symbolic_graph_iter_t;
/**
 * Return iterator for graph exec symbols from a graph.
 * @param graph The symbolic graph.
 * @param sources The sources for the graph.
 * @param source_size The size of the sources array. 0 to use default sources.
 * @param destinations The destinations for the graph.
 * @param destination_size The size of the destinations array. 0 to use default destinations.
 * @return The iterator for the symbolic graph.
 */
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_iter_t*) ccv_nnc_symbolic_graph_iter_new(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);
/**
 * Iterate to the next item.
 * @param iter The iterator for the symbolic graph.
 * @return 1 if successful, 0 if reached the end.
 */
CCV_WARN_UNUSED(int) ccv_nnc_symbolic_graph_iter_next(ccv_nnc_symbolic_graph_iter_t* const iter);
/**
 * Get the command and the name associated with the exec symbol.
 * @param iter The iterator for the symbolic graph.
 * @param cmd The pointer we are going to write command to.
 * @param hint The pointer we are going to write hint to.
 * @param flags The pointer we are going to write flags to.
 * @param name The pointer we are going to write name to.
 */
void ccv_nnc_graph_exec_symbol_from_iter(ccv_nnc_symbolic_graph_iter_t* const iter, ccv_nnc_cmd_t* const cmd, ccv_nnc_hint_t* const hint, int* const flags, char** const name);
/**
 * Get the inputs and outputs associated with the exec symbol.
 * @param iter The iterator for the symbolic graph.
 * @param inputs The pointer we are going to write input tensor symbols to.
 * @param input_size The pointer we are going to write input tensor symbol size to.
 * @param outputs The pointer we are going to write output tensor symbols to.
 * @param output_size The pointer we are going to write output tensor symbol size to.
 */
void ccv_nnc_tensor_symbol_io_from_iter(ccv_nnc_symbolic_graph_iter_t* const iter, ccv_nnc_tensor_symbol_t** const inputs, int* const input_size,  ccv_nnc_tensor_symbol_t** const outputs, int* const output_size);
/**
 * Free the iterator structure.
 * @param iter The iterator for the symbolic graph.
 */
void ccv_nnc_symbolic_graph_iter_free(ccv_nnc_symbolic_graph_iter_t* const iter);
/**
 * For a given tensor symbol, this method resolves to its local reference inside the given graph.
 * This is related to the sub-graph of symbolic graphs. A tensor symbol in the sub-graph can still have a
 * representation in the parent graph. This method used to find the local reference in any graph.
 * @param graph The symbolic graph.
 * @param tensor_symbol The tensor symbol we want to resolve.
 * @return A tensor symbol reference in the given graph.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_resolve(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol);
/**
 * Pass graph's tensor symbol into its sub graph. We will make the connection that the source tensor
 * symbol in the source symbolic graph is the destination tensor symbol in the destination symbolic graph.
 * The reason to do this inference is because a tensor symbol is local to a symbolic graph under the hood.
 * Although you can use tensor symbols from different graphs directly (it calls this method or the resolve
 * method above when create an execution node symbol), sometimes you need this method to do it manually.
 * @param src_graph The source symbolic graph.
 * @param dest_graph The destination symbolic graph.
 * @param src_tensor_symbol The tensor symbol we want to resolve.
 * @param dest_tensor_symbol The tensor symbol we want to resolve.
 */
void ccv_nnc_tensor_symbol_hookup(ccv_nnc_symbolic_graph_t* const src_graph, ccv_nnc_symbolic_graph_t* const dest_graph, const ccv_nnc_tensor_symbol_t src_tensor_symbol, const ccv_nnc_tensor_symbol_t dest_tensor_symbol);
/**
 * Set bypasses for a tensor symbol.
 * For case..of graphs, if the condition doesn't meet, we will skip the execution of a sub-graph.
 * However, in that case, we cannot express easily which output tensor corresponds to which input tensor.
 * This methods provides the way.
 * @param graph The symbolic graph.
 * @param symbol_map The pair of tensors array, source is the input tensor, destination is the output tensor.
 * @param symbol_map_size The size of the tensor pairs array.
 */
void ccv_nnc_tensor_symbol_set_bypasses(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);
/**
 * Fetch input / output for an exec symbol. For efficiency consideration, this returns pointer directly.
 * @param graph The symbolic graph.
 * @param symbol The execution node symbol reference.
 * @param inputs The pointer to store input tensor symbols array.
 * @param input_size The pointer to store the size of input tensor symbols array.
 * @param outputs The pointer to store output tensor symbols array.
 * @param output_size The pointer to store the size of output tensor symbols array.
 */
void ccv_nnc_graph_exec_symbol_io(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t symbol, const int** const inputs, int* const input_size, const int** const outputs, int* const output_size);
/**
 * Which exec symbol this is connected to. For efficiency consideration, this returns pointer directly.
 * @param graph The symbolic graph.
 * @param symbol The execution node symbol reference.
 * @param tos The pointer to store outgoing indexes of the execution nodes.
 * @param to_size the pointer to store the number of outgoing indexes.
 */
void ccv_nnc_graph_exec_symbol_to(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t symbol, const int** const tos, int* const to_size);
/**
 * Find the size allocated on the opaque tensor arena structure.
 * @param tensor_arena The tensor arena object generated through compilation.
 * @return The total allocated size in bytes.
 */
CCV_WARN_UNUSED(uint64_t) ccv_nnc_tensor_arena_size(const ccv_nnc_tensor_arena_t* const tensor_arena);
/**
 * Function prototype for tensor symbol creation callback.
 */
typedef void(*ccv_nnc_tensor_symbol_new_hook_f)(void* context, const ccv_nnc_tensor_symbol_t symbol, const ccv_nnc_tensor_param_t info, const char* const name);
/**
 * Hook into the call to ccv_nnc_tensor_symbol_new, return previous provided context if call into this method.
 * @param graph The symbolic graph.
 * @param hook The function to be called if a new tensor symbol created.
 * @param context The context associated with the callback function.
 */
void* ccv_nnc_tensor_symbol_new_hook(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_tensor_symbol_new_hook_f hook, void* context);
/**
 * Function prototype for tensor symbol alias creation callback.
 */
typedef void(*ccv_nnc_tensor_symbol_alias_new_hook_f)(void* context, const ccv_nnc_tensor_symbol_t symbol, const ccv_nnc_tensor_symbol_t from_symbol, const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC], const ccv_nnc_tensor_param_t info, const char* const name);
/**
 * Hook into the call to ccv_nnc_tensor_symbol_alias_new, return previous provided context if call into this method.
 * @param graph The symbolic graph.
 * @param hook The function to be called if a new tensor symbol alias created.
 * @param context The context associated with the callback function.
 */
void* ccv_nnc_tensor_symbol_alias_new_hook(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_tensor_symbol_alias_new_hook_f hook, void* context);
/**
 * Set the peer reference for tensor symbols. Peer reference for tensor symbols has very specific meanings.
 * For a backward pass involves sub-graphs. The commands in the sub-graph could reference to tensor symbols of
 * a different graph (its forward pass graph). That is not allowed (two graph has no ancestral relationship
 * cannot share a tensor symbol). So we create a new tensor symbol, but set the peer reference.
 * @param graph The symbolic graph.
 * @param tensor_symbol The tensor symbol in the current graph.
 * @param peer_tensor_symbol The tensor symbol in the peer graph.
 */
void ccv_nnc_tensor_symbol_set_peer(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol, const ccv_nnc_tensor_symbol_t peer_tensor_symbol);
/**
 * Function prototype for execution node symbol creation callback.
 */
typedef void(*ccv_nnc_graph_exec_symbol_new_hook_f)(void* context, const ccv_nnc_graph_exec_symbol_t symbol, const ccv_nnc_cmd_t cmd, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const char* const name);
/**
 * Hook into the call to ccv_nnc_graph_exec_symbol_new, return previous provided context if call into this method.
 * @param graph The symbolic graph.
 * @param hook The function to be called if a new execution node symbol created.
 * @param context The context associated with the callback function.
 */
void* ccv_nnc_graph_exec_symbol_new_hook(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_graph_exec_symbol_new_hook_f hook, void* context);
/**
 * Set the peer reference for exec. This is very similar to the one for concrete graph. A peer reference
 * of a backward pass execution node is its forward pass counterpart.
 * @param graph The symbolic graph.
 * @param exec_symbol The execution node symbol in the current graph.
 * @param peer_exec_symbol The peering execution node symbol.
 */
void ccv_nnc_graph_exec_symbol_set_peer(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec_symbol, const ccv_nnc_graph_exec_symbol_t peer_exec_symbol);

/** @} */

/** @} */

/**
 * @defgroup level_3_5 Level-3.5 API
 * @{
 */

/**
 * @defgroup level_3_5_autograd Automatic Differentiation
 * @{
 */

/**
 * Compute the backward graph, assuming the provided symbolic graph only contain the "forward" part from sources to destinations.
 * This effectively is called the "autograd" or automatic differentiation process (specifically, "reverse AD") in other libs.
 * For a expression y = f(x), to compute dx, x is the wrt_symbol, y is the f_symbol.
 * @param graph The symbolic graph.
 * @param f_symbols The tensor symbols array of the result (or loss).
 * @param f_symbol_size The size of the f symbols array.
 * @param wrt_symbols The tensor symbols array of the inputs.
 * @param wrt_symbol_size The size of the wrt symbols array.
 * @param sources The source execution nodes array for the computation.
 * @param source_size The size of the source nodes array.
 * @param destinations The destination execution nodes array for the computation.
 * @param destination_size The size of the destination nodes array.
 */
void ccv_nnc_symbolic_graph_backward(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t* const f_symbols, const int f_symbol_size, const ccv_nnc_tensor_symbol_t* const wrt_symbols, const int wrt_symbol_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);
/**
 * Get the symbol that contains the gradient. The list will be flushed if the ccv_nnc_symbolic_graph_backward function is called again.
 * @param graph The symbolic graph.
 * @param symbol The tensor symbol we want to retrieve its gradient (must be one of the wrt symbols or the f symbols).
 * @return A tensor symbol that represents the gradient.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_for_backward(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t symbol);
/**
 * Get the execution node symbol for a tensor symbol. This used to retrieve the execution node for a gradient tensor symbol.
 * @param graph The symbolic graph.
 * @param symbol The tensor symbol that represents the gradient (must be one of the wrt symbols).
 * @return A execution node symbol that generates the gradient.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_symbol_t) ccv_nnc_graph_exec_symbol_for_backward(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t symbol);

/** @} */

/**
 * @defgroup level_3_5_while While Loop
 * @{
 */

/**
 * @page symbolic_while Construct a "while" loop in a symbolic graph
 *
 * (This document was written in 2016, since then, Caffe2 added support for While loop (as sub-graph), similar
 * implementation added for ONNX as well.)
 *
 * In NNC, a computation graph cannot allow cycles. Thus, there is no flexible way to express loops.
 *
 * A little survey on this problem:
 *
 * * Caffe2 supports specific type of recurrent neural network.
 *
 * * TensorFlow as it stands, supports while construct. Its while construct is very straightforward, a body and
 *   a condition is provided, you can construct whatever graph as you want.
 *
 * * mxnet supports recurrent neural network by unrolling it into normal none-looped graph.
 *
 * * Theano supports "scan" ops, which is a terminable loop (with loop variant, known as sequence).
 *
 * * CNTK supports this with custom BrainScript. Within BrainScript, you can access the previous state in a
 *   function, therefore, effectively supports calling a method multiple times (looping over).
 *
 * Of above, Caffe2 and mxnet gave up on supporting generic loop for performance reasons. TensorFlow supports
 * generic while loop, with all the trouble it may introduce (see the Nested while loop bug in TensorFlow that
 * recently fixed). Theano picked a point seems pretty sweet, although there are limitations. CNTK's BrainScript
 * is a DSL, they can do whatever they want with the drawback now that they need to implement a language runtime.
 * TensorFlow, Theano and CNTK all support auto-differentiation over the while loop with tape (Wengert list).
 *
 * A simple way to support loop is to support conditional jump. In fact, conditional jump is a more generic way
 * of doing loops. However, if you put this into the consideration that fully differentiable computation graph
 * wanna to be supported, it is terrible. With conditional jump, it is really hard for you to know which tensor
 * is used where, thus keep track for reverse accumulation (backward propagation). There is no counter or
 * whatsoever, it is pretty hard to trace back on which line is executed how many times. Compounding this with
 * NNC's promise that as long as it shows on the graph can be "parallel" computed, it will be parallel computed,
 * it is close to impossible to track if conditional jump used in its raw form. Certain restrictions must be
 * applied to how to do the loop. The compromise comes from closer examination of NNC's preferences.
 *
 * NNC prefers to have the graph without cycles. It also prefers to be fully differentiable. Another important
 * criteria is that most functions in NNC require SSA (Static Single Assignment) representation. With these in
 * mind, supporting while loop has to be strict.
 *
 * Luckily, there are well-formalized way of supporting this in literature and practice. Because it is
 * well-formalized, translating this into existing NNC implementation is actually pretty straightforward. We
 * are going to introduce a special version of while loop. In literature that discussed about SSA, it may be
 * called parameterized loop. For us, it works like this:
 *
 * To construct a while loop for existing NNC graph, you need to be able to separate the existing graph into
 * two sub-graphs.
 *
 * The while-loop sub-graph (WL sub-graph) contains a set of incoming nodes (I-nodes), Condition false output
 * nodes (CFO-nodes) and end nodes (E-nodes). Each set have its own properties, but in short, all incoming edges
 * to the WL sub-graph connect to one of the I-nodes, but nothing else. All outgoing edges from the WL sub-graph
 * connect to one of the CFO-nodes, but nothing else. A nodes can be either a I-node, CFO-node or E-node,
 * non-exclusively.
 *
 * There are also 3 types of tensors used for all nodes in WL sub-graph: Input tensors (I-tensors) are tensors
 * that are inputs to some nodes, and will never be outputs. Output tensors (O-tensors) are tensors that are
 * outputs from some nodes, but never be inputs to any nodes. I-tensors can be outputs from some nodes that
 * outside of WL sub-graph. O-tensors can be inputs to some nodes that outside of WL sub-graph. Internal
 * tensors (IN-tensors) are not visible outside of WL sub-graph, therefore, they can be both inputs and outputs
 * of some nodes inside the sub-graph. Some tensors can be feedback into the WL sub-graph, given either
 * O-tensors or IN-tensors. A parameter map can be given in these cases to describe which maps to what.
 *
 * The way to drive a WL sub-graph like this: the WL sub-graph runs until all CFO-nodes are reached. At this
 * point, the while_f condition is checked. If true, we continue until all the end-nodes are reached. At this
 * point, we increase the counter, reconfigure the WL sub-graph with parameter map, and run from I-nodes all
 * over again. When reached all CFO-nodes, the condition is checked again, if false, WL sub-graph terminates,
 * and the graph continues from the nodes that are pointed by CFO-nodes.
 *
 * Given these constraints, doing automatic differentiation is not that hard any more. A WL sub-graph, from
 * the whole graph's point of view, is just a giant command supports both forward / backward operations, with
 * some extra information passed around in the form of userdata (tape).
 *
 * For WL sub-graph, we can continue to leverage the compile / backward function that already written for
 * symbolic graph as well.
 *
 * For compile function, we just need to take care of parameter maps (these need to be converted into binded
 * tensors).
 *
 * For backward function, we need to convert parameter maps from assigner (thus, y = x) to accumulator (x += y).
 *
 * This function will replace the nodes that it affects to one sub-graph node. Thus, how to drive this
 * sub-graph is opaque. Its backward form is opaque as well.
 *
 * There are no connection between its nodes and the outside graph nodes other than the three sets:
 *
 * 1. Incoming nodes, the set of nodes that contains the incoming edges from outside, they cannot have edges
 *    points by inside nodes. The sub-graph computation starts from these incoming nodes;
 *
 * 2. Condition false output nodes, when condition is false, we will break out of this while loop, these
 *    nodes pointing to the outside nodes, but no inside nodes;
 *
 * 3. End nodes, the set of nodes that marks the end of the while body, and after these nodes are executed,
 *    we will return to the incoming nodes. These end nodes shouldn't have any edges pointing to inside nodes
 *    (OK if end nodes are condition true output nodes as well);
 *
 * Since these will become a sub-graph (which, to its owner graph, just simple "node"), it will have inputs
 * and outputs. Besides that, the loop body needs to be parameterized to be SSA compliant (see:
 * https://www.cs.cmu.edu/~fp/courses/15411-f13/lectures/06-ssa.pdf). Thus, a list of body parameters need to
 * be provided.
 */

/**
 * @defgroup level_3_5_while_essentials While Loop Essentials
 * @{
 */

/**
 * The given tensors contains all the common / input / output tensors specified in the sub-graph.
 */
typedef int(*ccv_nnc_graph_while_f)(ccv_nnc_tensor_t* const* const inputs, const int input_size, const void* const data);
/**
 * Create a tensor tape that can be used to record for while loop or case..of.
 * @return A ccv_nnc_tensor_tape_t pointer.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_tape_t*) ccv_nnc_tensor_tape_new(void);
/**
 * Deallocate the tensor tape and all the memory it allocated.
 * @param tape The tensor tape object.
 */
void ccv_nnc_tensor_tape_free(ccv_nnc_tensor_tape_t* const tape);
/**
 * The API to operate on the symbolic graph is more involved than the concrete graph for while loops.
 * The reason is because symbolic graph operates in SSA form (static single assignment), therefore, the while
 * loops for the symbolic graph has to be parameterized.
 * @param graph The symbolic graph.
 * @param cmd The command idenfitier, can be either CCV_NNC_GRAPH_FORWARD or CCV_NNC_GRAPH_BACKWARD
 * @param while_graph The sub-graph to run the while loop.
 * @param name The name of the while loop. Optional.
 * @return A while loop execution symbol (backed by a sub-graph) of the giving graph.
 */
ccv_nnc_graph_exec_symbol_t ccv_nnc_symbolic_graph_while(ccv_nnc_symbolic_graph_t* const graph, const uint32_t cmd, ccv_nnc_symbolic_graph_t* const while_graph, const char* const name);
/**
 * Set the expression to be evaluated, and at which nodes to be evaluated.
 * @param while_graph The symbolic graph that will run the while loop.
 * @param while_expr The function pointer to the expression.
 * @param while_data A custom data provided to the expression evaluation function.
 * @param inputs The input tensor symbols array to the expression evaluation function.
 * @param input_size The size of the input tensor symbols array.
 * @param breakpoints The execution node symbols at which the while loop will pause, evaluate the expression, and choose to either break out or continue.
 * @param breakpoint_size The size of the execution node symbols array.
 */
void ccv_nnc_symbolic_graph_set_while_expr(ccv_nnc_symbolic_graph_t* const while_graph, const ccv_nnc_graph_while_f while_expr, const void* const while_data, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_graph_exec_symbol_t* const breakpoints, const int breakpoint_size);
/**
 * Set the loop carry parameters when reuse. (parameterized loop, these will be carried over to the next loop).
 * @param while_graph The symbolic graph that will run the while loop.
 * @param symbol_map A pair of tensor symbols array, where the source tensor symbol is the output tensor symbol in this loop, the destination tensor symbol is the input tensor symbol in the next loop.
 * @param symbol_map_size The size of the symbol map array.
 */
void ccv_nnc_symbolic_graph_set_carry_overs(ccv_nnc_symbolic_graph_t* const while_graph, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);
/**
 * Retrieve the special (magical) tensor symbol that retains the while loop counter (thus, dimension of 1x1x1, CCV_64S type).
 * @param while_graph The symbolic graph that will run the while loop.
 * @return A tensor symbol represents the implicit loop count.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_for_while_count(const ccv_nnc_symbolic_graph_t* const while_graph);
/**
 * Extract the sub-graph of the while loop from a symbol.
 * @param graph The symbolic graph.
 * @param while_symbol The execution node symbol.
 * @return The sub-graph that represents a while loop.
 */
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_from_while_symbol(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t while_symbol);
/**
 * Constructing looped concrete graph. Note that this interface is a little bit simpler than the one for symbolic
 * graph. The reason is that a concrete graph operates on allocated tensors, thus, there is no mapping of tensor
 * symbols between the parent graph and the while graph. (The reason to have a mapping in symbolic graphs is to
 * constraint the variable leaking between the sub graph and parent graph).
 * @param graph The concrete graph.
 * @param cmd The command idenfitier, can be either CCV_NNC_GRAPH_FORWARD or CCV_NNC_GRAPH_BACKWARD
 * @param while_graph The sub-graph to run the while loop.
 * @return A execution node that represents the sub-graph.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_while(ccv_nnc_graph_t* const graph, const uint32_t cmd, ccv_nnc_graph_t* const while_graph);
/**
 * Set the evaluated expression for the while loop. The while loop will break out if the expression evaluates to 0.
 * @param while_graph The concrete graph that will run the while loop.
 * @param while_expr The function pointer to the expression.
 * @param while_data A custom data provided to the expression evaluation function.
 * @param inputs The input tensors array to the expression evaluation function.
 * @param input_size The size of the input tensors array.
 * @param breakpoints The execution nodes at which the while loop will pause, evaluate the expression, and choose to either break out or continue.
 * @param breakpoint_size The size of the execution nodes array.
 */
void ccv_nnc_graph_set_while_expr(ccv_nnc_graph_t* const while_graph, const ccv_nnc_graph_while_f while_expr, const void* const while_data, ccv_nnc_tensor_t* const* const inputs, const int input_size, const ccv_nnc_graph_exec_t* const breakpoints, const int breakpoint_size);
/**
 * Get the special tensor for the while loop count. It contains one uint64_t value. We keep an implicit count
 * when evaluate the while loop and you can access it with this tensor.
 * @param while_graph The concrete graph that will run the while loop.
 * @return A special tensor that you can retrieve the loop count at .data.i64[0].
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_t) ccv_nnc_tensor_for_while_count(const ccv_nnc_graph_t* const while_graph);
/**
 * Retrieve the sub-graph from a execution node.
 * @param graph The concrete graph.
 * @param exec The execution node represents the sub-graph.
 * @return The sub-graph.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_t*) ccv_nnc_graph_from_while_exec(const ccv_nnc_graph_t* const graph, ccv_nnc_graph_exec_t exec);

/** @} */

/**
 * @defgroup level_3_5_while_others While Loop Others
 * @{
 */

/**
 * For a given tape on a given graph, update the input / output tensors so new version will be created (if needed).
 * @param tape The tensor tape object.
 * @param graph The concrete graph this tensor tape is executing in.
 * @param input_flags The flags associated with input tensors.
 * @param inputs The input tensors.
 * @param input_size The size of input tensors array.
 * @param output_flags The flags associated with output tensors.
 * @param outputs The output tensors.
 * @param output_size The size of output tensors array.
 */
void ccv_nnc_tensor_tape_io(ccv_nnc_tensor_tape_t* const tape, const ccv_nnc_graph_t* const graph, const int* const input_flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, const int* const output_flags, ccv_nnc_tensor_t* const* const outputs, const int output_size);
/**
 * Retrieve the number we associated with the execution node that recorded on the tape for a particular run of the graph.
 * @param tape The tensor tape object.
 * @param graph The concrete graph this tensor tape is executing in.
 * @param exec The execution node.
 * @return The number associated with the execution node.
 */
uint64_t ccv_nnc_tensor_tape_numbering(ccv_nnc_tensor_tape_t* const tape, const ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec);
/**
 * Set the number we associated with the execution node that recorded on the tape for a particular run of the graph.
 * @param tape The tensor tape object.
 * @param graph The concrete graph this tensor tape is executing in.
 * @param exec The execution node.
 * @param numbering The number associated with the execution node.
 */
void ccv_nnc_tensor_tape_set_numbering(ccv_nnc_tensor_tape_t* const tape, ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const uint64_t numbering);
/**
 * Augmented tensor to run a graph with while loop (An obvious example is dynamic RNN).
 */
typedef struct ccv_nnc_tensor_multiview_s {
	// This is an augmented ccv_nnc_tensor_view_t
	// Namely, it can point to multiple versions of tensors.
	int type; // This type is CCV_NNC_TENSOR_MULTI_VIEW
	// kind specified how the multi-version tensors stored.
	// See the comment on the follow up enums.
	uint8_t kind;
	uint16_t repeat;
	intptr_t anchor; // on which graph this multi-view tensor is wrapped. This helps to determine on which level the multi-view tensor should be unwrapped.
	// If this tensor points to a tensor view, data.u8 - offset is the real pointer start.
	off_t offset;
	struct ccv_nnc_tensor_multiview_s* p; // If this is wrapped with another multiview tensor. Get to the parent one.
	ccv_nnc_tensor_t* it; // Current tensor (tensor in use), this is updated along with the graph computation.
	// This is useful because by just traverse tv, I can get the latest up-to-date reference to this multi-view tensor.
	ccv_array_t* sp; // Synchronized tensor views. This corresponds to ccv_nnc_tensor_synchronize_to_multiview method, that records all the tensors registered for updates.
	ccv_nnc_tensor_t* _inline_data[4];
	ccv_nnc_tensor_t** _heap_data;
} ccv_nnc_tensor_multiview_t;
#define CCV_NNC_MULTIVIEW_DATA(x) ((x)->_heap_data ? (x)->_heap_data : (x)->_inline_data)
#define CCV_NNC_MULTIVIEW_PHI (intptr_t)0x1 /**< Denote this is a phi multi-view tensor. */

enum {
	CCV_NNC_MULTIVIEW_K0N = 0, /**< All of them are repeated. */
	CCV_NNC_MULTIVIEW_K1N = 1, /**< The first one is the first, the second one starts to repeat. (0111111...) */
};
#define CCV_NNC_MULTIVIEW_K01(x) ((x)->kind == CCV_NNC_MULTIVIEW_K0N && (x)->repeat == 1)
/**
 * Setup a tensor multiview with a given set of tensors.
 * A multiview tensor point to a list of tensors, and its access depends on the loop count.
 * For example, if we have a multiview tensor with list of [a, b, c, d], and kind is 1N, repeat is 3.
 * For loop count 0, 1, 2, 3, 4, 5, 6, the corresponding tensors used will be a, b, c, d, b, c. If kind
 * is 0N, and repeat is 4, it will be a, b, c, d, a, b.
 * @param data[] The pointer to the list of tensors the multiview object can point to.
 * @param kind Can be either CCV_NNC_MULTIVIEW_K0N or CCV_NNC_MULTIVIEW_K1N, basically whether to keep the initial tensor.
 * @param repeat The length of the repeat.
 * @param graph Which graph this multiview object attaches to.
 * @param tensor_multiview The tensor multiview object to be updated.
 */
void ccv_nnc_tensor_multiview(ccv_nnc_tensor_t* data[], const uint8_t kind, const uint16_t repeat, const ccv_nnc_graph_t* const graph, ccv_nnc_tensor_multiview_t* const tensor_multiview);
/**
 * Since tensor_multiview will never be allocated with *_new method, the *_free method simply frees anything that is dynamically allocated afterwards (such as the reference items).
 * @param tensor_multiview The tensor multiview object to be deallocated.
 */
void ccv_nnc_tensor_multiview_free(const ccv_nnc_tensor_multiview_t tensor_multiview);
/**
 * Setup a tensor as a reference to a tensor multiview, thus, when tensor multiview's tu (current tensor) updates, the tensor reference's data.u8 will get update as well (point to the same memory region as the tu).
 * @param tensor_multiview The tensor multiview object.
 * @param tensor The tensor that will be updated along with the multiview object.
 */
void ccv_nnc_tensor_synchronize_to_multiview(ccv_nnc_tensor_multiview_t* const tensor_multiview, ccv_nnc_tensor_t* const tensor);
/**
 * Send broadcast to subscribers of the multiview, call this in the beginning of exec.
 * @param tensor_multiview The tensor multiview object.
 */
void ccv_nnc_tensor_multiview_synchronize(ccv_nnc_tensor_multiview_t* const tensor_multiview);

/** @} */

/** @} */

/**
 * @defgroup level_3_5_case_of Branching
 * @{
 */

/**
 * @page symbolic_switch Construct "switch" control structure in symbolic graph
 *
 * Here I use the keyword case_of. To provide a "switch" control structure within NNC has some nice properties
 * even though you can simulate this with a while loop technically.
 *
 * 1. More optimal memory allocation: with "switch" control structure, memory can be multiplexed for each code
 *    path because they are mutually exclusive.
 *
 * 2. No tape should be used within each branch: if we simulate with a "while" loop, any results from within
 *    the "switch" statement has to be kept on the tape, which is inefficient because you don't need any tape
 *    for the "switch" statement other than record which path it is taken.
 *
 * The particular "switch" control structure provided here is a multi-way structured "switch". Each branch is a
 * sub-graph, so it is well-scoped. A node branch out based on the case_of condition return value to either of
 * the branch (numbering from 0 to n, -1 means no path taken). If no path taken, the output tensors will be
 * assigned with the default tensors and continue. Otherwise the computation within the sub-graph will be
 * carried out and the output tensors will be assigned with the tensors specified within that sub-graph and
 * continue.
 *
 * If we want to consider speculative execution in the future, we need to revisit our memory allocation scheme.
 */

/**
 * Function prototype to evaluate a branch expression.
 */
typedef int(*ccv_nnc_graph_case_of_f)(ccv_nnc_tensor_t* const* const inputs, const int input_size, const void* const data);
/**
 * Create a new case..of execution node symbol.
 * @param graph The symbolic graph.
 * @param cmd The command idenfitier, can be either CCV_NNC_GRAPH_FORWARD or CCV_NNC_GRAPH_BACKWARD
 * @param inputs The input tensor symbols array for the expression.
 * @param input_size The size of the input tensor symbols array.
 * @param symbol_map The pair of tensor symbols array where the source is the input tensor symbol and the destination is the output tensor symbol.
 * @param symbol_map_size The size of symbol map array.
 * @param name The name of the case..of graph. Optional.
 * @return A execution node symbol that represents the case..of graph.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_symbol_t) ccv_nnc_symbolic_graph_case_of_new(ccv_nnc_symbolic_graph_t* const graph, const uint32_t cmd, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size, const char* const name);
/**
 * Set the expression to be evaluated when choose which sub-graph to branch to.
 * @param graph The symbolic graph.
 * @param exec The execution node symbol that represents the case..of graph.
 * @param case_of The function pointer to evaluate.
 * @param case_of_data The data associated with the function pointer.
 */
void ccv_nnc_symbolic_graph_set_case_of_expr(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, ccv_nnc_graph_case_of_f case_of, const void* case_of_data);
/**
 * Set a sub-graph as one of the branch for the case..of graph.
 * @param graph The symbolic graph.
 * @param symbol The execution node symbol that represents the case..of graph.
 * @param case_graph The sub-graph for one of the branch.
 * @param case_of The index assigned to this sub-graph (expression returns this index to determine which sub-graph to execute).
 * @param symbol_map The pair of tensor symbols array where the source is the output tensor symbol of the sub-graph, and the destination is the output tensor symbol of the execution node symbol.
 * @param symbol_map_size The size of the symbol map array.
 */
void ccv_nnc_symbolic_graph_set_case_of(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t symbol, ccv_nnc_symbolic_graph_t* const case_graph, const int case_of, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);
/**
 * Create a new case..of execution node.
 * @param graph The concrete graph.
 * @param cmd The command idenfitier, can be either CCV_NNC_GRAPH_FORWARD or CCV_NNC_GRAPH_BACKWARD
 * @param inputs The input tensors array supplied to the expression.
 * @param input_size The size of the input tensors array.
 * @param outputs The output tensors array.
 * @param output_size The size of the output tensors array.
 * @return A execution node that represents the case..of graph.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_case_of_new(ccv_nnc_graph_t* const graph, const uint32_t cmd, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size);
/**
 * Set the expression to be evaluated when choose which sub-graph to branch to.
 * @param graph The concrete graph.
 * @param exec The execution node that represents the case..of graph.
 * @param case_of The function pointer to evaluate.
 * @param case_of_data The data associated with the function pointer.
 * @param offset A integer added to the expression output to help choose the index. Thus, real index = expression index + offset.
 */
void ccv_nnc_graph_set_case_of_expr(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_graph_case_of_f case_of, const void* case_of_data, const int offset);
/**
 * Set a sub-graph as one of the branch for the case..of graph.
 * @param graph The concrete graph.
 * @param exec The execution node that represents the case..of graph.
 * @param case_graph The sub-graph for one of the branch.
 * @param case_of The index assigned to this sub-graph (expression returns this index + offset to determine which sub-graph to execute).
 */
void ccv_nnc_graph_set_case_of(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_graph_t* const case_graph, const int case_of);

/** @} */

/**
 * @defgroup level_3_5_minimizer Gradient-based Optimization
 * @{
 */

/**
 * This is the comparable part to Caffe's solver or TensorFlow's optimizer. It took a step further than just
 * compute the gradient, but also apply the gradient to update parameters to minimize the loss.
 * @param graph The symbolic graph.
 * @param minimizer The wrapped command that represents a particular optimization strategy.
 * @param losses The tensor symbols array of losses.
 * @param loss_size The size of the loss symbols array.
 * @param parameters The parameter tensor symbols to optimize.
 * @param parameter_size The size of parameter symbols array.
 * @param inputs The additional input symbols we compute gradient against.
 * @param input_size The size of the additional input symbols array.
 * @param sources The source execution nodes array.
 * @param source_size The size of source nodes array.
 * @param destinations The destinations execution nodes array.
 * @param destination_size The size of destination nodes array.
 * @param gradients The tensor symbols that represents the gradient for update, should be the same size as the parameters array + input array size. This can be 0 (optional).
 * @param updated_parameters The tensor symbols that represents the updated parameters, should be the same size as the parameters array.
 * @param saved_aux The tensor symbols that is helpful for particular optimization strategy.
 * @param graph_exec_symbols The execution node symbols for the updates, should be the same size as the parameters array.
 */
void ccv_nnc_symbolic_graph_minimize(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_cmd_t minimizer, const ccv_nnc_tensor_symbol_t* const losses, const int loss_size, const ccv_nnc_tensor_symbol_t* const parameters, const int parameter_size, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size, ccv_nnc_tensor_symbol_t* const gradients, ccv_nnc_tensor_symbol_t* const updated_parameters, ccv_nnc_tensor_symbol_map_t* const saved_aux, ccv_nnc_graph_exec_symbol_t* const graph_exec_symbols);
/**
 * The number of extra saved aux per parameter only depends on the commands. For example, SGD with momentum requires 1 aux (for momentum).
 * Others require more.
 * @param minimizer The wrapped command that represents a particular optimization strategy.
 * @return the number of saved aux per parameter.
 */
CCV_WARN_UNUSED(int) ccv_nnc_minimizer_saved_aux_size(const ccv_nnc_cmd_t minimizer);

/** @} */

/**
 * @defgroup level_3_5_simplify Graph Simplification
 * @{
 */

/**
 * @page symbolic_simplify Symbolic graph simplification
 *
 * We make a distinction between graph simplifications and optimizations (autotune).
 *
 * Simplification: rewrite the graph and the resulting graph will have less nodes. This is done on the symbolic
 * graph only. Passes that is "simplification" include pruning, common sub-expression eliminations, constant
 * folding etc.
 *
 * Optimization (autotune): graph optimization can have more objectives. The most obvious objective is to reduce
 * computation time. For symbolic graph, passes that reduces computation time include data layout optimizations,
 * auto parallel etc (in normal optimization implementations, they have a cost model to guide the optimization.
 * NNC's implementation uses a cost database that profiles the time cost on the device to guide the optimization.
 * We call it autotune to distinguish with the normal optimization passes because we need device profile data).
 * There could be other objectives, for example, in many deep learning applications, reducing memory footprint
 * can be desirable. However, as always in computer science, memory and time is a typical trade-off. Memory
 * optimization almost always results longer computation time, and the objective is to trade between these two
 * with a bias term (in other frameworks such as TensorFlow, the memory optimizer uses a list of "cheap ops" to
 * bias between the time and memory footprint).
 *
 * For graph optimizations, it can happen on both the symbolic graph level as well as the concrete graph level.
 * For NNC, symbolic graph is already very explicit (data layout, device allocation and data transfer between
 * devices / nodes, even the command backend can all be specified on the symbolic graph), however, some
 * information is unknown until it is compiled down to concrete graph (tensor addresses, tensor initialization
 * etc.), and since graph optimizations need all the information to optimize. Keeping the flexibility to do
 * optimization on both symbolic and concrete graph level seems reasonable.
 */

enum {
	/**
	 * If two commands generated the same outputs, all the places where the newer output used will be replaced by
	 * the old output. Later on the graph pruning stage, the command that generate the newer output will be
	 * eliminated.
	 */
	CCV_NNC_SIMPLIFY_COMMON_SUBEXPRESSION_ELIMINATION,
	/**
	 * For the given outputs, eliminate unused input tensors, and then eliminate graph execs that don't contribute
	 * to the outputs.
	 */
	CCV_NNC_SIMPLIFY_GRAPH_PRUNING,
	/**
	 * For CCV_NNC_DATA_TRANSFER, if the input / output is the same (on the same device, no alias), we can skip.
	 * Similarly, if it is on the same device, but alias of some, for some cases we can skip as well (if neither
	 * are carry overs, bypasses etc.)
	 */
	CCV_NNC_SIMPLIFY_DATA_TRANSFER_OPT,
	/**
	 * Combine a few smaller ops into bigger one. For now, this functionality is limited. I can only address ops
	 * that are sequential.
	 */
	CCV_NNC_SIMPLIFY_OPS_FUSION,
	// CCV_NNC_SIMPLIFY_CONSTANT_FOLDING, // This currently is not supported, because we don't have efficient way to express constant in symbolic graph.
};
/**
 * Simplify a graph with given list of passes, in that particular order.
 * Note, when a graph is simplified, its sources / destinations are changed as well.
 * @param graph The symbolic graph.
 * @param passes The array of passes we are going to apply.
 * @param pass_size The size of the passes array.
 * @param outputs The output tensor symbols we want to retain (we are going to prune any execution nodes that is not related to these outputs).
 * @param output_size The size of the output array.
 * @param sources The source execution node symbols array.
 * @param source_size The size of source node symbols array.
 * @param destinations The destinations execution node symbols array.
 * @param destination_size The size of destination node symbols array.
 */
void ccv_nnc_symbolic_graph_simplify(ccv_nnc_symbolic_graph_t* const graph, const int* const passes, const int pass_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);

/** @} */

/**
 * @defgroup level_3_5_parallel Automatic Graph Parallelization
 * @{
 */

enum {
	/**
	 * Op for reducer / allreducer. Currently only supports sum.
	 */
	CCV_NNC_PARALLEL_REDUCE_OP_SUM,
};

/**
 * Turn the existing graph to be capable to run on several devices with different data inputs at parallel.
 * With this method, additional tensor symbols will be created that runs on different devices. That has
 * been said, there are concepts of "broadcast" and "reduce". "broadcast" tensor symbols will be copied to
 * different devices, while "reduce" tensors will be summed from different devices to the default device.
 * "allreducer" concept is simpler. The allreduce operation will be performed on these tensors and then
 * be used on different devices again.
 *
 * Limitations: right now, the way to reduce / allreduce tensors only supports "sum". The data parallel
 * only supports GPU, thus, the nodes will be duplicated are GPU computations and GPU memory backed
 * tensors. Also, right now, the tensors to be broadcasted / allreduced / reduced should have no aliases.
 *
 * @param graph The symbolic graph.
 * @param parallel Number of devices we want to run on. 0 will use all devices available. 1 will skip.
 * @param broadcasts The tensor symbols to be broadcasted.
 * @param broadcast_size The size of the broadcast tensor symbols array.
 * @param allreducers The tensor symbols that to be allreduced.
 * @param allreducer_size The size of the allreducer tensor symbols array.
 * @param reducers The tensor symbols to be reduced.
 * @param reducer_size The size of the reducer tensor symbols array.
 * @param reduce_op_type The reduce op for reducer / allreducer.
 * @param sources The source execution node symbols array.
 * @param source_size The size of source node symbols array.
 * @param destinations The destinations execution node symbols array.
 * @param destination_size The size of destination node symbols array.
 */
void ccv_nnc_symbolic_graph_data_parallel(ccv_nnc_symbolic_graph_t* const graph, const int parallel, const ccv_nnc_tensor_symbol_t* const broadcasts, const int broadcast_size, const ccv_nnc_tensor_symbol_t* const allreducers, const int allreducer_size, const ccv_nnc_tensor_symbol_t* const reducers, const int reducer_size, const int reduce_op_type, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);

/**
 * Apply LSSC memory compression algorithm to the convolution activations. This will compress the activation
 * layer for convolution, therefore, save the overall memory usage during training time.
 *
 * @param graph The symbolic graph.
 * @param sources The source execution node symbols array.
 * @param source_size The size of source node symbols array.
 * @param destinations The destinations execution node symbols array.
 * @param destination_size The size of destination node symbols array.
 */
void ccv_nnc_symbolic_graph_memory_compression(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);
/**
 * Get the symbol that is on a device other than the default one. The list will be flushed if the
 * ccv_nnc_symbolic_graph_data_parallel function is called again.
 * @param graph The symbolic graph.
 * @param symbol The tensor symbol we want to retrieve its counterparts on a different device.
 * @param device_id The device numeric id for this symbol.
 * @return A tensor symbol that is on a different device.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_copy(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t symbol, const int device_id);
/**
 * Get the execution node that is on a device other than the default one. The list will be flushed
 * if the ccv_nnc_symbolic_graph_data_parallel function is called again.
 * @param graph The symbolic graph.
 * @param symbol The execution node we want to retrieve its counterparts on a different device.
 * @param device_id The device numeric id for this symbol.
 * @return A execution node that is on a different device.
 */
CCV_WARN_UNUSED(ccv_nnc_graph_exec_symbol_t) ccv_nnc_graph_exec_symbol_copy(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t symbol, const int device_id);

/** @} */

/** @} */

/**
 * @defgroup level_4 Level-4 API
 * @{
 */

/**
 * Opaque pointer to the dynamic graph structure.
 */
typedef struct ccv_nnc_dynamic_graph_s ccv_nnc_dynamic_graph_t;

/**
 * Masquerade this as if it is a on stack variable, there is a heap allocation but managed by the dynamic graph.
 * The fact that ccv_nnc_tensor_variable_t is a pointer is an implementation detail. It should be treated as an
 * opaque type throughout. We may later extends this to be some on-stack information or even just a uid.
 */
typedef struct ccv_nnc_tensor_variable_s* ccv_nnc_tensor_variable_t;

/**
 * Create a dynamic graph.
 * @return A newly created dynamic graph.
 */
CCV_WARN_UNUSED(ccv_nnc_dynamic_graph_t*) ccv_nnc_dynamic_graph_new(void);

/** @cond ALL */
// Get a new tensor variable.
CCV_WARN_UNUSED(ccv_nnc_tensor_variable_t) ccv_nnc_tensor_variable_new_impl(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_param_t info);
#define CCV_NNC_TENSOR_VARIABLE_NEW_X_1(graph) ccv_nnc_tensor_variable_new_impl(graph, ccv_nnc_tensor_auto)
#define CCV_NNC_TENSOR_VARIABLE_NEW_X_SEL(_1, _2, _FX, ...) _FX
// Making so that this new method can take parameters for both no parameter or with tensor_param.
#define ccv_nnc_tensor_variable_new(graph, ...) CCV_NNC_TENSOR_VARIABLE_NEW_X_SEL(graph, ##__VA_ARGS__, ccv_nnc_tensor_variable_new_impl, CCV_NNC_TENSOR_VARIABLE_NEW_X_1)(graph, ##__VA_ARGS__)
CCV_WARN_UNUSED(ccv_nnc_tensor_variable_t) ccv_nnc_tensor_constant_new_impl(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_param_t info);
#define CCV_NNC_TENSOR_CONSTANT_NEW_X_1(graph) ccv_nnc_tensor_constant_new_impl(graph, ccv_nnc_tensor_auto)
#define CCV_NNC_TENSOR_CONSTANT_NEW_X_SEL(_1, _2, _FX, ...) _FX
// Making so that this new method can take parameters for both no parameter or with tensor_param.
#define ccv_nnc_tensor_constant_new(graph, ...) CCV_NNC_TENSOR_CONSTANT_NEW_X_SEL(graph, ##__VA_ARGS__, ccv_nnc_tensor_constant_new_impl, CCV_NNC_TENSOR_CONSTANT_NEW_X_1)(graph, ##__VA_ARGS__)
/** @endcond */

/**
 * Create a new tensor variable that is an alias of a given tensor variable.
 * @param graph The dynamic graph.
 * @param tensor_variable The tensor variable we are going to alias from.
 * @param ofs The offset on each of the dimension.
 * @param inc The line size of each dimension.
 * @param info The tensor parameters for the new alias.
 * @return New tensor variable that is an alias.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_variable_t) ccv_nnc_tensor_variable_alias_new(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable, const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC], const ccv_nnc_tensor_param_t info);
/**
 * Get the underlying tensor for the tensor variable. The tensor allocation may be performed when calling this
 * method.
 * @param graph The dynamic graph.
 * @param tensor_variable The tensor variable to get the underlying tensor.
 * @return The underlying tensor.
 */
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_from_variable(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable);
/**
 * Set a tensor on the tensor variable. Tensor variable doesn't take over the life-cycle management of the tensor
 * (in similar way as the tensor binds).
 * @param graph The dynamic graph.
 * @param tensor_variable The tensor variable to set.
 * @param tensor The tensor that is going to be associated with the tensor variable.
 */
void ccv_nnc_tensor_variable_set(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable, ccv_nnc_tensor_t* const tensor);
/**
 * Execute a command with given tensor variables, the output is in the output tensor variables.
 * @param graph The dynamic graph.
 * @param cmd The wrapped command.
 * @param hint The hint associated with the command.
 * @param flags A reserved field for flags.
 * @param inputs The input tensor variables array.
 * @param input_size The size of the input tensor variables array.
 * @param outputs The output tensor variables array.
 * @param output_size The size of the output tensor variables array.
 */
int ccv_nnc_dynamic_graph_exec(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, const ccv_nnc_tensor_variable_t* const inputs, const int input_size, ccv_nnc_tensor_variable_t* const outputs, const int output_size);
/**
 * Compute the gradient of given tensor, with respect to the f. Thus, df / dt.
 * @param dynamic_graph The dynamic graph.
 * @param f_variable The output losses.
 * @param df_optional The custom gradient for f. If not provided, will default to 1.
 * @param inputs The input variables.
 * @param input_size The size of the input variables array.
 * @param outputs The gradients with respect to the inputs.
 * @param output_size The size of the outputs array. Should be equal to the input_size.
 */
void ccv_nnc_dynamic_graph_backward(ccv_nnc_dynamic_graph_t* const dynamic_graph, const ccv_nnc_tensor_variable_t f_variable, const ccv_nnc_tensor_variable_t df_optional, const ccv_nnc_tensor_variable_t* const inputs, const int input_size, ccv_nnc_tensor_variable_t* const outputs, const int output_size);
/**
 * Apply one step of minimization (most likely, a gradient descent) to the parameters with a given loss (or
 * losses).
 * @param dynamic_graph The dynamic graph.
 * @param minimizer The wrapped command that represents a particular optimization strategy.
 * @param losses The losses we are trying to minimize.
 * @param loss_size The size of the losses array.
 * @param dlosses_optional The custom gradient for losses. If not provided, will default to 1.
 * @param parameters The parameters to update.
 * @param parameter_size The size of parameters array.
 * @param saved_aux The aux variables to faciliate the minimizer. See ccv_nnc_minimizer_saved_aux_size.
 */
void ccv_nnc_dynamic_graph_minimize(ccv_nnc_dynamic_graph_t* const dynamic_graph, const ccv_nnc_cmd_t minimizer, const ccv_nnc_tensor_variable_t* const losses, const int loss_size, const ccv_nnc_tensor_variable_t* const dlosses_optional, ccv_nnc_tensor_variable_t* const parameters, const int parameter_size, ccv_nnc_tensor_variable_t* const saved_aux);
/**
 * Dispose a tensor variable. You cannot do any computation against this tensor variable afterwards.
 * @param graph The dynamic graph.
 * @param tensor_variable The tensor variable to be disposed.
 */
void ccv_nnc_tensor_variable_free(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable);
/**
 * Free the dynamic graph.
 * @param graph The dynamic graph.
 */
void ccv_nnc_dynamic_graph_free(ccv_nnc_dynamic_graph_t* const graph);
/**
 * Generate output that can be parsed by GraphViz (DOT language).
 * @param graph The dynamic graph.
 * @param flags Either CCV_NNC_SHORT_DOT_GRAPH or CCV_NNC_LONG_DOT_GRAPH
 * @param out The output file stream.
 */
void ccv_nnc_dynamic_graph_dot(const ccv_nnc_dynamic_graph_t* const graph, const int flags, FILE* out);

/** @} */

/**
 * @defgroup level_5 Level-5 API
 * @{
 */

/**
 * @page dataframe What is "dataframe" in ML?
 *
 * A large part of machine learning consists of go through data, process them to a shape / form that makes sense,
 * and pass that into the model to train. Deep learning frameworks such as TensorFlow or PyTorch provides some
 * dataset APIs for this purpose. It is convenient for these frameworks because by being Python, people can use
 * Pandas to process the data. In Pandas, this is called Dataframe, which again, imitates R language.
 *
 * Another interesting observation comes from recent (2018) release of Create ML framework from Apple. It provides
 * a very close to Pandas style data process API (MLDataTable) but in Swift. This implementation is important because
 * it provides a survey point other than Python.
 *
 * Comparing to Python, Swift is a stronger typed language. Though all being high-level, they all have pretty good
 * string support (of course!), operator overloading, and polymorphism. String support makes column naming natural,
 * Operator overloading makes conditioning and filtering easier, and polymorphism makes column type representation
 * straight-forward. These, unfortunately, are the challenges I need to face when implementing in C with the eye
 * towards that later the similar ideas can be implemented on top on a high-level language based on this one.
 *
 * It seems I haven't answered the most crucial question yet: what's special about these data process APIs? It is
 * easier to answer this to first see what Pandas or MLDataTable does.
 *
 * * They both represent data as tables. Each column represents different type of the data (time, nd-array, scalar
 *   or string). As such, they both have API to add / remove / rename columns, and load tabular data from disk.
 *
 * * They both provide API to filter (remove / add) rows, and derive new column from existing columns.
 *
 * * Pandas provides more API for data alignment (merge columns from different tables into one table), and compute
 *   statistics (group rows by some criteria, and compute min / max / std / mean within that group).
 *
 * * MLDataTable provides API to batching data (random split) which covered in TensorFlow / PyTorch's Dataset API
 *   as well.
 *
 * It turns out when you have a noisy dataset, these functionalities are useful to remove unwanted data quickly.
 * If you have a relatively clean dataset, it also allows you to prepare data in a more elegant way. For NNC,
 * the interesting requirements are:
 *
 * 1. Represents scalars, tensors, string as columns; columns can be named.
 *
 * 2. New columns can be derived, from existing ones.
 *
 * 3. Rows can be filtered, grouped, and statistics can be computed.
 *
 * 4. Columns can be aligned, with some given indexes.
 *
 * 5. All these can be done efficiently, on a scale of hundreds of Gigabytes data.
 */

/**
 * @defgroup level_5_dataframe Dataframe API
 * @{
 */

/**
 * A data enumeration function to supply data for given row indexes.
 */
typedef void (*ccv_cnnp_column_data_enum_f)(const int column_idx, const int* const row_idxs, const int row_size, void** const data, void* const context, ccv_nnc_stream_context_t* const stream_context);
/**
 * A destructor for data.
 */
typedef void (*ccv_cnnp_column_data_deinit_f)(void* const data, void* const context);
/**
 * A destructor for context.
 */
typedef void (*ccv_cnnp_column_data_context_deinit_f)(void* const context);
/**
 * Column data.
 */
typedef struct {
	int stream_type; /**< The type of stream context for this column. Each column only compatible with one stream type. */
	ccv_cnnp_column_data_enum_f data_enum; /**< The data enumeration function for this column. */
	ccv_cnnp_column_data_deinit_f data_deinit; /**< The deinit function that will be used to destroy the data. */
	void* context; /**< The context go along with this column. */
	ccv_cnnp_column_data_context_deinit_f context_deinit; /**< The deinit function that will be used to destroy the context. */
} ccv_cnnp_column_data_t;
/**
 * An opaque structure point to the dataframe object.
 */
typedef struct ccv_cnnp_dataframe_s ccv_cnnp_dataframe_t;
/**
 * Create a dataframe object with given column data.
 * @param column_data The column data that can be loaded.
 * @param column_size The size of column data array.
 * @param row_count The number of rows in this dataframe.
 */
CCV_WARN_UNUSED(ccv_cnnp_dataframe_t*) ccv_cnnp_dataframe_new(const ccv_cnnp_column_data_t* const column_data, const int column_size, const int row_count);
/**
 * Add a new column to the dataframe.
 * @param dataframe The dataframe object to add column to.
 * @param data_enum The data provider function for the new column.
 * @param stream_type The type of stream context for this derived column.
 * @param data_deinit The deinit function will be used to destroy the derived data.
 * @param context The context that can be used to generate new column.
 * @param context_deinit The deinit function will be used to destroy the context.
 * @return The new column index.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_add(ccv_cnnp_dataframe_t* const dataframe, ccv_cnnp_column_data_enum_f data_enum, const int stream_type, ccv_cnnp_column_data_deinit_f data_deinit, void* const context, ccv_cnnp_column_data_context_deinit_f context_deinit);
/**
 * A map function that takes the data from multiple columns and derive new data out of it.
 */
typedef void (*ccv_cnnp_column_data_map_f)(void*** const column_data, const int column_size, const int batch_size, void** const data, void* const context, ccv_nnc_stream_context_t* const stream_context);
/**
 * Derive a new column out of existing columns in the dataframe.
 * @param dataframe The dataframe object that contains existing columns.
 * @param map The map function used to derive new column from existing columns.
 * @param stream_type The type of stream context for this derived column.
 * @param data_deinit The deinit function will be used to destroy the derived data.
 * @param column_idxs The columns that will be used to derive new column.
 * @param column_idx_size The size of existing columns array.
 * @param context The context that can be used to generate new column.
 * @param context_deinit The deinit function will be used to destroy the context.
 * @return The new column index.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_map(ccv_cnnp_dataframe_t* const dataframe, ccv_cnnp_column_data_map_f map, const int stream_type, ccv_cnnp_column_data_deinit_f data_deinit, const int* const column_idxs, const int column_idx_size, void* const context, ccv_cnnp_column_data_context_deinit_f context_deinit);
/**
 * Shuffle an existing dataframe.
 * @param dataframe The dataframe that is about to be shuffled.
 */
void ccv_cnnp_dataframe_shuffle(ccv_cnnp_dataframe_t* const dataframe);
/**
 * Query row count of the dataframe.
 * @param dataframe The dataframe we want to query row count.
 * @return The row count of the dataframe.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_row_count(ccv_cnnp_dataframe_t* const dataframe);
/**
 * A reduce function that takes multiple rows of one column, and reduce to one row.
 */
typedef void (*ccv_cnnp_column_data_reduce_f)(void** const input_data, const int batch_size, void** const output_data, void* const context, ccv_nnc_stream_context_t* const stream_context);
/**
 * Reduce a dataframe by batch size. Thus, n rows are reduced to 1 row per reduce function on
 * one specific column. This will also reduce the multi-column dataframe down to 1 column
 * by selecting the one column to reduce.
 * @param dataframe The dataframe that is about to be reduced.
 * @param reduce The reduce function used to reduce n rows into 1.
 * @param data_deinit The deinit function will be used to destroy the derived data.
 * @param column_idx The column we selected to reduce.
 * @param batch_size How many rows will be reduced to 1 row from the original data.
 * @param context The context that can be used in reduce function.
 * @param context_deinit The deinit function will be used to destroy the context.
 * @return The reduced dataframe.
 */
CCV_WARN_UNUSED(ccv_cnnp_dataframe_t*) ccv_cnnp_dataframe_reduce_new(ccv_cnnp_dataframe_t* const dataframe, ccv_cnnp_column_data_reduce_f reduce, ccv_cnnp_column_data_deinit_f data_deinit, const int column_idx, const int batch_size, void* const context, ccv_cnnp_column_data_context_deinit_f context_deinit);
/**
 * Extract a value out of a struct. Assuming the data points to a struct. This method extract
 * n-offset value of that struct. For example, if you have struct { ccv_nnc_tensor_t* a; ccv_nnc_tensor_t* b; } S;
 * if you want to extract the b tensor to a different column, you can call this function with
 * offsetof(S, b).
 * @param dataframe The dataframe object to be extracted.
 * @param column_idx The column that we want to extract value of.
 * @param offset The offset. For example, offsetof(S, b).
 * @return The new column that contains the extracted value.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_extract_value(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const off_t offset);
/**
 * Make a tuple out of columns specified. Thus, the new derived column will contains a tuple
 * with data from all the columns specified here. Tuple here represented as void* tuple[], an
 * array of void* pointers.
 * @param dataframe The dataframe that will contain the new column.
 * @param column_idxs The columns to be tupled.
 * @param column_idx_size The number of columns.
 * @return The derived column with the tuple.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_make_tuple(ccv_cnnp_dataframe_t* const dataframe, const int* const column_idxs, const int column_idx_size);
/**
 * The size of the tuple. It is equal to the number of columns we specified. The behavior of
 * calling this method on a column that is not a tuple is undefined.
 * @param dataframe The dataframe that contains the tuple column.
 * @param column_idx The tuple column we are going to inspect.
 * @return The tuple size of the column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_tuple_size(const ccv_cnnp_dataframe_t* const dataframe, const int column_idx);
/**
 * Extract a data out of a tuple.
 * @param dataframe The dataframe that will contain the new column.
 * @param column_idx The column that is a tuple.
 * @param index The index into the tuple.
 * @return The derived column with the extracted value.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_extract_tuple(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const int index);
/**
 * The opaque pointer to the iterator.
 */
typedef struct ccv_cnnp_dataframe_iter_s ccv_cnnp_dataframe_iter_t;
/**
 * Get a new iterator of the dataframe.
 * @param dataframe The dataframe object to iterate through.
 * @param column_idxs The columns that will be iterated.
 * @param column_idx_size The size of columns array.
 * @return The opaque iterator object.
 */
CCV_WARN_UNUSED(ccv_cnnp_dataframe_iter_t*) ccv_cnnp_dataframe_iter_new(ccv_cnnp_dataframe_t* const dataframe, const int* const column_idxs, const int column_idx_size);
/**
 * Get the next item from the iterator.
 * @param iter The iterator to go through.
 * @param data_ref The output for the data.
 * @param column_idx_size The size of the data_ref array.
 * @param stream_context The stream context to extract data asynchronously.
 * @return 0 if the iteration is successful, -1 if it is ended.
 */
int ccv_cnnp_dataframe_iter_next(ccv_cnnp_dataframe_iter_t* const iter, void** const data_ref, const int column_idx_size, ccv_nnc_stream_context_t* const stream_context);
/**
 * Prefetch next item on the iterator with the given stream context. You can call this method multiple times
 * to prefetch multiple items ahead of time.
 * @param iter The iterator to go through.
 * @param prefetch_count How much ahead we should advance for.
 * @param stream_context The stream context to extract data asynchronously.
 * @return 0 if the prefetch is successful, -1 if it is ended.
 */
int ccv_cnnp_dataframe_iter_prefetch(ccv_cnnp_dataframe_iter_t* const iter, const int prefetch_count, ccv_nnc_stream_context_t* const stream_context);
/**
 * Set the cursor of the iterator. When set to 0, the iterator effectively restarts.
 * @param iter The iterator to go through.
 * @param idx The index of the cursor.
 * @return 0 if it is successful, -1 if it is not (exceed the range).
 */
int ccv_cnnp_dataframe_iter_set_cursor(ccv_cnnp_dataframe_iter_t* const iter, const int idx);
/**
 * Free the dataframe iterator object.
 * @param iter The dataframe iterator to be freed.
 */
void ccv_cnnp_dataframe_iter_free(ccv_cnnp_dataframe_iter_t* const iter);
/**
 * Free the dataframe object.
 * @param dataframe The dataframe object to be freed.
 */
void ccv_cnnp_dataframe_free(ccv_cnnp_dataframe_t* const dataframe);

/** @} */

/**
 * @defgroup level_5_dataframe_add_ons Dataframe Add-ons
 * @{
 */

/**
 * Turn a ccv_array_t to a dataframe object.
 * @param array The array we want to turn into a dataframe object.
 * @return The new dataframe object.
 */
CCV_WARN_UNUSED(ccv_cnnp_dataframe_t*) ccv_cnnp_dataframe_from_array_new(ccv_array_t* const array);
/**
 * Derive a new column that copies a tensor array from given column to the derived column on GPU.
 * @param dataframe The dataframe object that get the derived column.
 * @param column_idx The original column contains tensor array on CPU.
 * @param tensor_offset Only copy as outputs[i] = inputs[i + tensor_offset].
 * @param tensor_size How many tensors in the tensor array.
 * @param device_id The device we want to copy the tensors to.
 * @return The index of the newly derived column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_copy_to_gpu(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const int tensor_offset, const int tensor_size, const int device_id);
/**
 * Derive a new column by executing a generic command.
 * @param dataframe The dataframe object that get the derived column.
 * @param column_idx The original column contains tensor array.
 * @param cmd The command for this operation.
 * @param hint The hint to run the command.
 * @param flags The flags with the command.
 * @param input_offset Use inputs[i + input_offset] to inputs[i + input_offset + input_size - 1] as the inputs
 * @param input_size How many tensors in the input array.
 * @param output_params The parameters for the outputs.
 * @param output_size How many tensors in the output array.
 * @param stream_type The type of stream context we are going to use.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_cmd_exec(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, const int input_offset, const int input_size, const ccv_nnc_tensor_param_t* const output_params, const int output_size, const int stream_type);
/**
 * Add a new column contains some tensors. This will add a new column that each row is the tensor specified
 * as the parameters. It comes handy when you want to have some auxiliary tensors along with each row.
 * @param dataframe The dataframe object that get the new column.
 * @param params The parameters for the tensors.
 * @return The index of the newly added column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_add_aux(ccv_cnnp_dataframe_t* const dataframe, const ccv_nnc_tensor_param_t params);
/**
 * Read image off a said column. That column should contain the filename (as char array). The new column
 * will contain the ccv_dense_matrix_t / ccv_nnc_tensor_t (both are toll-free bridging) of the image.
 * @param dataframe The dataframe object that loads the images.
 * @param column_idx The column which contains the filename.
 * @param structof The offset to the filename (as char array) from that column. For example, the column
 *        could be a struct and filename could be one of the field. In that case, you can pass offsetof(S, filename)
 * @return The index of the newly derived column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_read_image(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const off_t structof);
/**
 * The structure to describe how to apply random jitter to the image.
 */
typedef struct {
	float contrast; /**< The random contrast, the final contrast will be [1 / (1 + contrast), 1 + contrast] */
	float saturation; /**< The saturation, the final saturation will be [1 / (1 + saturation), 1 + saturation] */
	float brightness; /**< The brightness, the final brightness will be between [1 / (1 + brightness), 1 + brightness] */
	float lighting; /**< AlexNet style PCA based image jitter */
	float aspect_ratio; /**< Stretch aspect ratio between [1 / (1 + asepct_ratio), 1 + aspect_ratio] */
	int symmetric; /**< Apply random flip on x-axis (around y-axis */
	int seed; /**< The seed for random generator. */
	int center_crop; /**< Enable crop to the center (otherwise do random crop). */
	struct {
		int min; /**< The minimal dimension of resize */
		int max; /**< The maximal dimension of resize. The final resize can be computed from min + (max - min) * random_unit */
	} resize;
	struct {
		int rows; /**< The height of the final image. */
		int cols; /**< The width of the final image. */
	} size;
	struct {
		int x; /**< The extra random offset on x-axis. */
		int y; /**< The extra random offset on y-axis. */
	} offset;
	struct {
		float mean[3]; /**< Normalize the image with mean. */
		float std[3];/**< Normalize the image with std. pixel = (pixel - mean) / std */
	} normalize;
} ccv_cnnp_random_jitter_t;
/**
 * Apply random jitter on a image to generate a new image.
 * @param dataframe The dataframe object that contains the original image.
 * @param column_idx The column which contains the original image.
 * @param datatype The final datatype of the image. We only support CCV_32F right now.
 * @param random_jitter The random jitter parameters to be applied to.
 * @return The index of the newly derived column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_image_random_jitter(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const int datatype, const ccv_cnnp_random_jitter_t random_jitter);
/**
 * Generate a one-hot tensor off the label from a struct.
 * @param dataframe The dataframe object that contains the label.
 * @param column_idx The column which contains the label (as int).
 * @param structof The offset to the label (as int) from that column. For example, the column
 *        could be a struct and label could be one of the field. You can pass offsetof(S, filename)
 * @param range The range of the label, from [0...range - 1]
 * @param onval The value when it hit.
 * @param offval The value for the others.
 * @param datatype The datatype of the tensor.
 * @param format The format of the tensor.
 * @return The index of the newly derived column.
 */
CCV_WARN_UNUSED(int) ccv_cnnp_dataframe_one_hot(ccv_cnnp_dataframe_t* const dataframe, const int column_idx, const off_t structof, const int range, const float onval, const float offval, const int datatype, const int format);
/**
 * Batch multiple tensors in a column into one tensor. This method can take multiple columns, which
 * will result a tuple of tensors. Each tensor in the tuple is a batched one from a given column.
 * @param dataframe The dataframe contains the columns of tensors to be batched.
 * @param column_idxs The columns that contain the tensors.
 * @param column_idx_size The number of columns that contain the tensors.
 * @param batch_count How many tensors in one column to be batched together.
 * @param group_count We can generate many groups of batched tensor. For example, if you have column A, B, C, each
 *        have different tensors. If group_count is 1, the result tuple will be (A_b, B_b, C_b). If group count is
 *        2, the result tuple will be (A_b1, B_b1, C_b1, A_b2, B_b2, C_b2). A_b1 etc. will still contain the same
 *        number of batch_count tensors.
 * @param format The result format of the tensor. We support simply transformation NCHW <=> NHWC with the source tensor.
 * @return The newly created dataframe with the 0-th column is the tuple of batched tensors.
 */
CCV_WARN_UNUSED(ccv_cnnp_dataframe_t*) ccv_cnnp_dataframe_batching_new(ccv_cnnp_dataframe_t* const dataframe, const int* const column_idxs, const int column_idx_size, const int batch_count, const int group_count, const int format);

/** @} */

/**
 * @page model Models, layers, and Keras
 *
 * With Keras API in mind, this model implementation essentially is a light-weight way to group neural network layers
 * together. This is a rare case in NNC (or ccv in general) where Object-Oriented programming makes sense. I borrowed
 * heavily from Objective-C / C++ to implement this Object-Oriented interface.
 *
 * Now back to elaboration of the Model interface. It is specifically designed with Keras in mind, asking question:
 * If we are going to build Keras high-level API in any languages (Ruby, Python, Swift, Julia), what's the underlying
 * C interface would look like? Here is your answer (hint: it looks very much like just Python Keras API).
 *
 * A model consists of a set of inputs and outputs. This sounds very much like what "Command" is in Level-1 APIs,
 * however, they are different: a model is stateful. For example, a convolution command takes 3 inputs: image, kernel
 * weight and bias, has 1 output: image. A convolution model takes 1 input: image, and 1 output: image. kernel weight
 * and bias are internal states to the model (in Keras, it is called "layer" for convolution, and model means a set of
 * layers. In NNC, that kind of differentiation feels superficial, therefore, a layer is a model).
 *
 * A model can be combined, and a new model can be a combination of other models.
 *
 * The simpler composed model is the sequential model. A sequential model is a model that consists a sequence of models
 * that contains one input and one output. The output of the earlier model feed into the later one, thus, a sequential
 * evaluation path.
 */

/**
 * @defgroup level_5_model Model API
 * @{
 */

/**
 * model type is an abstract type, you won't interact with a naked model ever.
 */
typedef struct ccv_cnnp_model_s ccv_cnnp_model_t;
/**
 * With this type, now in NNC, we have 4 types that represents a "tensor":
 * ccv_nnc_tensor_t / ccv_nnc_tensor_view_t / ccv_nnc_tensor_multiview_t: a concrete tensor with memory allocated.
 * ccv_nnc_tensor_symbol_t: a symbol representation of a tensor, with its data layout, device affinity, and type
 *                          specified.
 * ccv_nnc_tensor_variable_t: in dynamic graph, this represents a concrete tensor with memory allocated, but also
 *                            associated with a recorded execution.
 * ccv_cnnp_model_io_t: this is the most flexible one. No data layout, device affinity or type specified, the format
 *                      has to be c / h / w, no batch size needed. This is a handle used by model API to associates
 *                      model inputs / outputs.
 */
typedef struct ccv_cnnp_model_io_s* ccv_cnnp_model_io_t;
/**
 * Create a naked input.
 * @return A ccv_cnnp_model_io_t represents an input.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_io_t) ccv_cnnp_input(void);
/**
 * This method mimics Keras callable for model (thus, override __call__ method in Python class).
 * @param model A model that we can apply a set of inputs to get one output.
 * @param inputs The set of inputs.
 * @param input_size The size of inputs array.
 * @return A ccv_cnnp_model_io_t that represents the output of the given model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_io_t) ccv_cnnp_model_apply(ccv_cnnp_model_t* const model, const ccv_cnnp_model_io_t* const inputs, const int input_size);
/**
 * This method name is deceiving. It return a composed model, not a naked model.
 * This composed model takes set of inputs, and run through various other models to arrive at
 * the set of outputs.
 * @param inputs The set of inputs.
 * @param input_size The size of inputs array.
 * @param outputs The set of outputs.
 * @param output_size The size of outputs array.
 * @return A composed model that takes inputs, and generate the outputs.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_model_new(const ccv_cnnp_model_io_t* const inputs, const int input_size, const ccv_cnnp_model_io_t* const outputs, const int output_size);
/**
 * This method returns a sequential model, which composed from a sequence of models.
 * @param models The list of models, that takes one input, and emit one output, feeding into the subsequent one.
 * @param model_size The size of the list.
 * @return A composed model that applies these models one by one in sequence.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_sequential_new(ccv_cnnp_model_t* const* const models, const int model_size);
/**
 * Prepare the model to be trained, the input specifies the batch size etc.
 * Input size technically is not needed, here is a safety check.
 * @param model The model to be compiled.
 * @param inputs The tensor parameters for the model's inputs, that can be used to derive all tensor shapes.
 * @param input_size The size of the inputs array.
 * @param minimizer The wrapped command that represents a particular optimization strategy.
 * @param loss The wrapped command that computes the loss function.
 */
void ccv_cnnp_model_compile(ccv_cnnp_model_t* const model, const ccv_nnc_tensor_param_t* const inputs, const int input_size, const ccv_nnc_cmd_t minimizer, const ccv_nnc_cmd_t loss);
/**
 * Generate output that can be parsed by GraphViz (DOT language).
 * @param model The composed model.
 * @param flags Either CCV_NNC_SHORT_DOT_GRAPH or CCV_NNC_LONG_DOT_GRAPH
 * @param outs The output file streams.
 * @param out_size The size of output file stream array.
 */
void ccv_cnnp_model_dot(const ccv_cnnp_model_t* const model, const int flags, FILE** const outs, const int out_size);
/**
 * Fit a model to a given input / output. This is a combination of running ccv_cnnp_model_evaluate /
 * ccv_cnnp_model_backward / ccv_cnnp_model_apply_gradients. The difference is that when calling
 * individual functions, the graph is compiled piece by piece, thus, is less efficient than calling
 * ccv_cnnp_model_fit directly. However, having the separate functions makes this implementation much
 * more versatile, for example, can accumulate gradients for multiple batches, or using custom gradients
 * etc.
 * @param model The composed model.
 * @param inputs The input tensors.
 * @param input_size The size of the input tensors array.
 * @param fits The target tensors.
 * @param fit_size The size of the target tensors array.
 * @param outputs The actual outputs from the model.
 * @param output_size The size of the outputs array.
 * @param stream_context The stream where the fit can be executed upon.
 */
void ccv_cnnp_model_fit(ccv_cnnp_model_t* const model, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const fits, const int fit_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);

/**
 * The parameters for how evaluation should behave.
 */
typedef struct {
	int requires_grad; /**< Whether we need to keep intermediate results for gradient computations. */
	int enable_outgrad; /**< Whether we can compute outflow gradients when call ccv_cnnp_model_backward later. */
	int is_test; /**< Whether we evaluate it as test, or just as forward pass of the training process. */
} ccv_cnnp_evaluate_param_t;
/**
 * Evaluate model with output.
 * @param model The composed model.
 * @param params The parameters for how evaluation should behave.
 * @param inputs The input tensors.
 * @param input_size The size of the input tensors array.
 * @param outputs The actual outputs from the model.
 * @param output_size The size of the outputs array.
 * @param stream_context The stream where the evaluation can be executed upon.
 */
void ccv_cnnp_model_evaluate(ccv_cnnp_model_t* const model, const ccv_cnnp_evaluate_param_t params, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, ccv_nnc_stream_context_t* const stream_context);
/**
 * Based on the input gradients, compute the output gradients (w.r.t. the inputs). This also adds trainable gradients.
 * @param model The composed model.
 * @param ingrads The input gradients.
 * @param ingrad_size The size of the input gradients array.
 * @param outgrads The output gradients (w.r.t. the inputs).
 * @param outgrad_size The size of the output gradients array.
 * @param stream_context The stream where the gradient computation can be executed upon.
 */
void ccv_cnnp_model_backward(ccv_cnnp_model_t* const model, ccv_nnc_tensor_t* const* const ingrads, const int ingrad_size, ccv_nnc_tensor_t* const* const outgrads, const int outgrad_size, ccv_nnc_stream_context_t* const stream_context);
/**
 * Apply the computed gradients to the trainable tensors.
 * @param model The composed model.
 * @param stream_context The stream where the gradient computation can be executed upon.
 */
void ccv_cnnp_model_apply_gradients(ccv_cnnp_model_t* const model, ccv_nnc_stream_context_t* const stream_context);
enum {
	/**
	 * This is the default flag, if the model is not initialized, will attempt to read from the disk.
	 * Otherwise, will persist existing parameters to disk.
	 */
	CCV_CNNP_MODEL_CHECKPOINT_READ_WRITE,
	/**
	 * Only read parameters out of disk, even it is already initialized.
	 */
	CCV_CNNP_MODEL_CHECKPOINT_READ_ONLY,
	/**
	 * Only write parameters to disk.
	 */
	CCV_CNNP_MODEL_CHECKPOINT_WRITE_ONLY,
};
/**
 * This method checkpoint the given model. If the model is initialized, it will persist all parameters
 * to the given file path. If it is not initialized, this method will try to load tensors off the
 * disk.
 * @param model The composed model.
 * @param fn The file name.
 * @param flags Whether we perform read / write on this checkpoint, or read only / write only.
 */
void ccv_cnnp_model_checkpoint(ccv_cnnp_model_t* const model, const char* const fn, const int flags);
/**
 * Apply data parallel to the composed model. This method has to be called before we call either
 * evaluate or fit and after the model is compiled.
 * @param model The composed model.
 * @param parallel Number of devices we want to run on. 0 will use all devices available. 1 will skip.
 */
void ccv_cnnp_model_set_data_parallel(ccv_cnnp_model_t* const model, const int parallel);
/**
 * Apply memory compression to the composed model. The memory compression technique can reduce memory
 * usage up to 75% comparing with raw mix-precision model during training time.
 * @param model The composed model.
 * @param memory_compression Whether to enable the memory compression (1 - enable, 0 - disable (default))
 */
void ccv_cnnp_model_set_memory_compression(ccv_cnnp_model_t* const model, const int memory_compression);
/**
 * This method set the max workspace size. If the graph is already compiled. It will re-run
 * autotune to use the new workspace size to find the best algorithm.
 * @param model The composed model.
 * @param workspace_size The size in bytes that we can use as workspace (scratch memory).
 */
void ccv_cnnp_model_set_workspace_size(ccv_cnnp_model_t* const model, size_t workspace_size);
/**
 * Simple structure for group of command and the index for the variable.
 */
typedef struct {
	const ccv_nnc_cmd_t cmd;
	const int index; // The input index.
} ccv_cnnp_trainable_index_t;
/**
 * The setter function prototype for ccv_cnnp_model_set_minimizer. This is useful because it helps to
 * set different minimizer parameters for different trainables. The example would be disable weight decay
 * for bias / scale variables. If I expand this idea a bit, I can also support for different trainables,
 * have entirely different minimizer  function. However, I haven't seen anything that can be trained with
 * different minimizer (most likely due to epoch updates learn rate, therefore, it is hard to manipulate
 * proper learn rate if different minimizers are used for different trainables at the same time). If
 * there is a model does that, I can add that (need some thinking though). Because we cannot attach names
 * to it (hmm, in retrospect, we probably should), the way we identify the trainables is to through which
 * node it is used (by the command type), and in which position. Also, it is only interesting if the
 * trainable is the input of some command. Therefore, only show it if it is an input.
 */
typedef ccv_nnc_cmd_t(*ccv_cnnp_model_minimizer_set_f)(const ccv_cnnp_model_t* const model, const ccv_cnnp_trainable_index_t* const indexes, const int index_size, const void* const context);
/**
 * Set a new minimizer for the model. This is useful when you need to update learn rate for stochastic
 * gradient descent for example. This method can be called any time during the training process (after
 * compilation).
 * @param model The composed model.
 * @param minimizer The wrapped command that represents a new optimization strategy.
 * @param minimizer_setter The function to be called to return minimizer for a particular trainable.
 * @param context The context passed to the minimizer setter function.
 */
void ccv_cnnp_model_set_minimizer(ccv_cnnp_model_t* const model, const ccv_nnc_cmd_t minimizer, const ccv_cnnp_model_minimizer_set_f minimizer_setter, const void* const context);
/**
 * Get the default stream from a compiled model. If the model is not compiled, the default stream is
 * 0.
 * @param model The composed model.
 * @return The default stream for this model.
 */
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_cnnp_model_default_stream(const ccv_cnnp_model_t* const model);
/**
 * Get the allocated memory size (exclude workspace) from a compiled model. If the model is not compiled
 * the size is 0.
 * @param model The composed model.
 * @return The number of bytes for memory allocated.
 */
CCV_WARN_UNUSED(uint64_t) ccv_cnnp_model_memory_size(const ccv_cnnp_model_t* const model);
/**
 * Free a given model.
 * @param model The composed model.
 */
void ccv_cnnp_model_free(ccv_cnnp_model_t* const model);

enum {
	CCV_CNNP_ACTIVATION_NONE,
	CCV_CNNP_ACTIVATION_RELU,
	CCV_CNNP_ACTIVATION_SOFTMAX,
};

enum {
	CCV_CNNP_NO_NORM,
	CCV_CNNP_BATCH_NORM,
};

typedef struct {
	int no_bias; /**< No bias term. */
	int norm; /**< The normalizations can be applied after activation such as CCV_CNNP_BATCH_NORM. */
	int activation; /**< The activations  can be applied for the output, such as CCV_CNNP_ACTIVATION_RELU or CCV_CNNP_ACTIVATION_SOFTMAX. */
	ccv_nnc_hint_t hint; /**< The hint for a particular operation */
} ccv_cnnp_param_t;
/**
 * Add multiple input tensors together.
 * @return A model that can be applied with multiple inputs, and generate output that is a sum of the inputs.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_add(void);
/**
 * Concatenate input tensors together.
 * @return A model that can be applied with multiple inputs, and generate output that is a concatenation of the inputs.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_concat(void);
/**
 * An identity layer that takes input and do nothing pass it as the output. Realistically, we use this
 * because we want to apply some normalization / activation function on top of the input.
 * @param params Parameters (such as hint and activation or norm).
 * @return A model that takes input and pass it as output.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_identity(const ccv_cnnp_param_t params);
/**
 * A convolution model.
 * @param groups The number of kernel groups in the model.
 * @param filters The total number of filters in the model (filters = groups * per group filters).
 * @param kdim The dimensions of the kernel.
 * @param params Other parameters (such as hint and activation or norm).
 * @return A convolution model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_convolution(const int groups, const int filters, const int kdim[CCV_NNC_MAX_DIM_ALLOC], const ccv_cnnp_param_t params);
/**
 * A dense layer model.
 * @param count The output dimension.
 * @param params Other parameters (such as hint and activation or norm).
 * @return A dense layer model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_dense(const int count, const ccv_cnnp_param_t params);
/**
 * A max pool model.
 * @param kdim The pooling window dimension.
 * @param params Other parameters (such as hint and activation or norm).
 * @return A max pool model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_max_pool(const int kdim[CCV_NNC_MAX_DIM_ALLOC], const ccv_cnnp_param_t params);
/**
 * An average pool model.
 * @param kdim The pooling window dimension.
 * @param params Other parameters (such as hint and activation or norm).
 * @return An average pool model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_average_pool(const int kdim[CCV_NNC_MAX_DIM_ALLOC], const ccv_cnnp_param_t params);
/**
 * Reshape an input into a different dimension.
 * @param dim The new dimension for the input.
 * @return A reshape layer model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_reshape(const int dim[CCV_NNC_MAX_DIM_ALLOC]);
/**
 * Flatten an input tensor into a one dimensional array.
 * @return A flatten layer model.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_flatten(void);

enum {
	CCV_CNNP_IO, /**< The parameter is a ccv_cnnp_io_t. */
	CCV_CNNP_NO_TENSOR, /**< The parameter is not used. */
	CCV_CNNP_INIT_SHARED_TENSOR, /**< The parameter is a provided tensor for initialization. */
	CCV_CNNP_INIT_SHARED_TENSOR_AS_TRAINABLE, /**< The parameter is a provided tensor that can be updated. */
};

typedef struct {
	int type; /**< The type of the parameter, could be CCV_CNNP_IO, NO_TENSOR, INIT_SHARED_TENSOR, or INIT_SHARED_TENSOR_TRAINABLE */
	ccv_nnc_tensor_t* tensor; /**< The tensor that is going to be used for initialization. */
} ccv_cnnp_tensor_param_t;
/**
 * A generic model based on the command. If the tensors are labeled as ccv_cnnp_io_t, it will participate
 * as the input / output of the model. If it is a init tensor, the model will use this tensor for that parameter.
 * More over, if it is marked as trainable, that tensor will be differentiated against when you call
 * ccv_cnnp_model_fit. This model however doesn't take over ownership of the tensor. You should manage the life
 * cycle of the given tensor and it is your responsibility to make sure they outlive the model. Also, all inputs and
 * outputs marked as init tensors will be shared if you reuse this model in other places.
 * @param cmd The command to generate this model.
 * @param hint The hint to run the command.
 * @param flags The flags with the command.
 * @param inputs A list of ccv_cnnp_tensor_param_t identify each input as either a init tensor or a ccv_cnnp_io_t.
 * @param input_size The size of input list.
 * @param outputs A list of types identify each output as ccv_cnnp_io_t or a none tensor.
 * @param output_size The size of the outputs. There is no need to give ccv_cnnp_tensor_param_t for outputs because
 *        all of them are CCV_CNNP_IO type.
 * @return A model based on the given command.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_cmd_exec(const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, const ccv_cnnp_tensor_param_t* const inputs, const int input_size, const int* const outputs, const int output_size);

typedef struct {
	ccv_nnc_tensor_symbol_t symbol; /**< The tensor symbol this is reference to. */
	int type; /**< The type of the parameter, could be CCV_CNNP_IO, INIT_SHARED_TENSOR, or INIT_SHARED_TENSOR_TRAINABLE */
	ccv_nnc_tensor_t* tensor; /**< The tensor that is going to be used for initialization. */
} ccv_cnnp_tensor_symbol_param_t;
/**
 * A generic model based on the symbolic graph we provided. A list of tensor symbols are labeled whether it
 * is ccv_cnnp_io_t or not (we identify whether this is a input or output based on whether it is in the graph).
 * If it is not, we init it with a given tensor. If it is marked as trainable, that tensor will be differentiated
 * against when you call ccv_cnnp_model_fit. The model doesn't take ownership over the init tensors. You are
 * responsible to make sure the init tensors outlive the model until the initialization occurred. Also, these
 * tensors will be shared if the model is reused.
 * @param graph The symbolic graph that is our blue print for this model.
 * @param tensor_symbol_params The list of tensor symbol parameters that labels a given symbol.
 * @param tensor_symbol_param_size The size of the list.
 * @param inputs The inputs to this graph. We can figure out which ones are inputs, but this gives us the order.
 * @param input_size The size of the input list.
 * @param outputs The outputs from this graph. We can figure out which ones are outputs, but this gives us the order.
 * @param output_size The size of the output list.
 * @return A model based on the given symbolic graph.
 */
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_graph(const ccv_nnc_symbolic_graph_t* const graph, const ccv_cnnp_tensor_symbol_param_t* const tensor_symbol_params, const int tensor_symbol_param_size, ccv_nnc_tensor_symbol_t* const inputs, const int input_size, ccv_nnc_tensor_symbol_t* const outputs, const int output_size);

/** @} */

/** @} */

#endif
